"""
LLM Judge Evaluator

ä½¿ç”¨LLMä½œä¸ºè¯„å§”è¯„ä¼°æ•°æ®ç”Ÿæˆè´¨é‡ï¼Œæ”¯æŒè‡ªå®šä¹‰ç»´åº¦å’Œå­—æ®µæ˜ å°„
"""

import json
import time
from typing import List, Dict, Any, Optional
from datetime import datetime
from hello_agents.core.llm import HelloAgentsLLM
from hello_agents.evaluation.benchmarks.data_generation_Universal.evaluation_config import EvaluationConfig


class UniversalLLMJudgeEvaluator:
    """LLM Judgeè¯„ä¼°å™¨ - æ”¯æŒè‡ªå®šä¹‰ç»´åº¦"""

    def __init__(
        self,
        llm: Optional[HelloAgentsLLM] = None,
        judge_model: str = "gpt-4o",
        eval_config: Optional[EvaluationConfig] = None,
        field_mapping: Optional[Dict[str, str]] = None,
        data_format: str = "standard"
    ):
        """
        åˆå§‹åŒ–LLM Judgeè¯„ä¼°å™¨

        Args:
            llm: LLMå®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°å®ä¾‹
            judge_model: è¯„å§”æ¨¡å‹åç§°
            eval_config: è¯„ä¼°é…ç½®ï¼Œé»˜è®¤ä½¿ç”¨æ ‡å‡†4ä¸ªç»´åº¦
            field_mapping: å­—æ®µæ˜ å°„ï¼ˆå¦‚{"problem": "é¢˜ç›®"}ï¼‰
            data_format: æ•°æ®æ ¼å¼ç±»å‹ï¼ˆ"standard", "csv", "custom"ï¼‰
        """
        self.llm = llm or HelloAgentsLLM(model=judge_model)
        self.judge_model = judge_model
        self.eval_config = eval_config or EvaluationConfig()
        self.field_mapping = field_mapping
        self.data_format = data_format
        
    def evaluate_single(
        self,
        problem: Dict[str, Any],
        reference: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªé—®é¢˜"""
        # é€‚é…å­—æ®µ
        problem = self._adapt_fields(problem)
        if reference:
            reference = self._adapt_fields(reference)

        start_time = time.time()

        # æ„å»ºè¯„ä¼°æç¤ºè¯ï¼ˆæ”¯æŒè‡ªå®šä¹‰ç»´åº¦ï¼‰
        prompt = self._build_evaluation_prompt(problem, reference)

        # è°ƒç”¨LLMè¿›è¡Œè¯„ä¼°
        messages = [{"role": "user", "content": prompt}]
        response = self.llm.invoke(messages)

        # è§£æè¯„ä¼°ç»“æœ
        scores = self._parse_evaluation_response(response)

        # è®¡ç®—æ€»åˆ†
        total_score = sum(scores.values()) / len(scores) if scores else 0.0

        execution_time = time.time() - start_time

        return {
            "problem_id": problem.get("id", "unknown"),
            "scores": scores,
            "total_score": total_score,
            "evaluation_text": response,
            "execution_time": execution_time
        }
    
    def evaluate_batch(
        self,
        problems: List[Dict[str, Any]],
        references: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """æ‰¹é‡è¯„ä¼°é—®é¢˜"""
        print(f"\nğŸ¯ å¼€å§‹LLM Judgeè¯„ä¼°")
        print(f"   è¯„ä¼°ç»´åº¦: {', '.join(self.eval_config.get_dimension_names())}")
        print(f"   è¯„ä¼°æ•°é‡: {len(problems)}")

        results = []
        for idx, problem in enumerate(problems):
            print(f"\n   è¯„ä¼°è¿›åº¦: {idx + 1}/{len(problems)}")

            # æå–é—®é¢˜IDå¹¶ç§»é™¤ç»´åº¦ä¿¡æ¯ï¼ˆå¦‚å°† writing-accuracy-001 è½¬æ¢ä¸º writing-001ï¼‰
            problem_id = problem.get('id', 'unknown')
            display_id = self._extract_display_id(problem_id)
            print(f"   ğŸ“Š {display_id}:")

            reference = references[idx] if references and idx < len(references) else None
            result = self.evaluate_single(problem, reference)
            results.append(result)

            # æ˜¾ç¤ºå„ç»´åº¦çš„è¯„åˆ†
            max_scale = self.eval_config.dimensions[0].scale if self.eval_config.dimensions else 5
            for dimension_name, score in result['scores'].items():
                print(f"      â€¢ {dimension_name}: {score:.2f}/{max_scale}")

            # æ˜¾ç¤ºå¹³å‡åˆ†
            print(f"      âœ å¹³å‡åˆ†: {result['total_score']:.2f}/{max_scale}")

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        metrics = self._compute_metrics(results)

        return {
            "results": results,
            "metrics": metrics,
            "evaluation_date": datetime.now().isoformat(),
            "judge_model": self.judge_model,
            "dimensions": self.eval_config.get_dimension_names(),
            "num_problems": len(problems)
        }
    
    def _build_evaluation_prompt(
        self,
        problem: Dict[str, Any],
        reference: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        æ„å»ºè¯„ä¼°æç¤ºè¯ï¼ˆæ”¯æŒè‡ªå®šä¹‰ç»´åº¦ï¼‰

        åŠ¨æ€ç”Ÿæˆæç¤ºè¯ï¼ŒåŒ…å«æ‰€æœ‰è‡ªå®šä¹‰ç»´åº¦
        """
        # æ„å»ºä¸»è¦å†…å®¹
        prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹å†…å®¹çš„è´¨é‡ã€‚

é¢˜ç›®ï¼š
{problem.get('problem', '')}

ç­”æ¡ˆï¼š
{problem.get('answer', '')}
"""

        if problem.get('solution'):
            prompt += f"\nè§£ç­”ï¼š\n{problem.get('solution', '')}\n"

        if reference:
            prompt += f"""
ã€å‚è€ƒå†…å®¹ã€‘
é¢˜ç›®ï¼š
{reference.get('problem', '')}

ç­”æ¡ˆï¼š
{reference.get('answer', '')}
"""
            if reference.get('solution'):
                prompt += f"\nè§£ç­”ï¼š\n{reference.get('solution', '')}\n"

        # åŠ¨æ€ç”Ÿæˆè¯„ä¼°ç»´åº¦è¯´æ˜
        prompt += "\nè¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œè¯„åˆ†ï¼ˆæ¯ä¸ªç»´åº¦1-5åˆ†ï¼‰ï¼š\n\n"

        # æ„å»ºç»´åº¦åˆ—è¡¨ï¼Œä½¿ç”¨ <strong> æ ‡è®°å¼ºè°ƒç»´åº¦åç§°
        for idx, dimension in enumerate(self.eval_config.dimensions, 1):
            prompt += f"{idx}. <strong>{dimension.name}</strong>ï¼š{dimension.description}\n"

        # ç”ŸæˆJSONæ ¼å¼è¯´æ˜
        dimension_names = self.eval_config.get_dimension_names()
        json_template = {name: 5 for name in dimension_names}
        json_template["comments"] = "è¯„ä»·ç†ç”±"

        prompt += f"\nè¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºè¯„åˆ†ï¼š\n```json\n{json.dumps(json_template, ensure_ascii=False, indent=4)}\n```"

        return prompt
    
    def _parse_evaluation_response(self, response: str) -> Dict[str, float]:
        """è§£æLLMè¯„ä¼°å“åº”"""
        try:
            # æå–JSONéƒ¨åˆ†
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
            else:
                json_str = response.strip()

            data = json.loads(json_str)

            # æå–è¯„åˆ†ï¼ˆä»…æå–ç»´åº¦åˆ†æ•°ï¼Œå¿½ç•¥commentsç­‰ï¼‰
            scores = {}
            dimension_names = self.eval_config.get_dimension_names()

            for dim_name in dimension_names:
                scores[dim_name] = float(data.get(dim_name, 3.0))

            return scores

        except Exception as e:
            print(f"âš ï¸ è§£æè¯„ä¼°å“åº”å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤è¯„åˆ†
            return {dim: 3.0 for dim in self.eval_config.get_dimension_names()}
    
    def _compute_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        if not results:
            return {}

        # è®¡ç®—å„ç»´åº¦å¹³å‡åˆ†
        dimension_names = self.eval_config.get_dimension_names()
        dimension_scores = {dim: [] for dim in dimension_names}
        total_scores = []

        for result in results:
            total_scores.append(result["total_score"])
            for dim in dimension_names:
                if dim in result["scores"]:
                    dimension_scores[dim].append(result["scores"][dim])

        metrics = {
            "average_total_score": sum(total_scores) / len(total_scores),
            "dimension_averages": {
                dim: sum(scores) / len(scores)
                for dim, scores in dimension_scores.items()
                if scores
            },
            "pass_rate": sum(1 for s in total_scores if s >= 3.5) / len(total_scores),
            "excellent_rate": sum(1 for s in total_scores if s >= 4.5) / len(total_scores)
        }

        return metrics

    def _adapt_fields(self, problem: Dict[str, Any]) -> Dict[str, Any]:
        """é€‚é…å­—æ®µï¼ˆå¦‚æœæä¾›äº†field_mappingï¼‰"""
        if not self.field_mapping:
            return problem

        adapted = {}
        for standard_key, source_key in self.field_mapping.items():
            if source_key in problem:
                adapted[standard_key] = problem[source_key]

        # ä¿ç•™å…¶ä»–å­—æ®µ
        for key, value in problem.items():
            if key not in self.field_mapping.values():
                adapted[key] = value

        # ç¡®ä¿åŸºæœ¬å­—æ®µå­˜åœ¨
        for key in ["id", "problem", "answer"]:
            if key not in adapted and key in problem:
                adapted[key] = problem[key]

        return adapted

    def _extract_display_id(self, problem_id: str) -> str:
        """
        ä»é—®é¢˜IDä¸­æå–æ˜¾ç¤ºç”¨çš„IDï¼Œç§»é™¤ç»´åº¦ä¿¡æ¯
        ä¾‹å¦‚ï¼šwriting-accuracy-001 -> writing-001
        """
        parts = problem_id.split('-')
        if len(parts) >= 2:
            # å¦‚æœæ˜¯ writing-accuracy-001 è¿™æ ·çš„æ ¼å¼
            if parts[-1].isdigit():  # æœ€åä¸€éƒ¨åˆ†æ˜¯æ•°å­—
                # æ£€æŸ¥ä¸­é—´éƒ¨åˆ†æ˜¯å¦æ˜¯ç»´åº¦åï¼ˆé€šå¸¸æ˜¯å•ä¸ªå•è¯ï¼‰
                if len(parts) >= 3 and parts[1] in ['accuracy', 'coherence', 'creativity', 'engagement', 'clarity', 'completeness']:
                    # ç§»é™¤ç»´åº¦éƒ¨åˆ†ï¼Œä¿ç•™ writing-001
                    return f"{parts[0]}-{parts[-1]}"
        return problem_id

    def export_results(
        self,
        results: Dict[str, Any],
        output_path: str
    ):
        """å¯¼å‡ºè¯„ä¼°ç»“æœ"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… è¯„ä¼°ç»“æœå·²ä¿å­˜: {output_path}")

