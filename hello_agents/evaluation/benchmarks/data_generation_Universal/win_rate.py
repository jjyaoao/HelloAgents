"""
Win Rate Evaluator

é€šè¿‡æˆå¯¹å¯¹æ¯”è®¡ç®—èƒœç‡ï¼Œæ”¯æŒè‡ªå®šä¹‰ç»´åº¦å’Œå­—æ®µæ˜ å°„
"""

import json
import time
import random
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from hello_agents.core.llm import HelloAgentsLLM
from hello_agents.evaluation.benchmarks.data_generation_Universal.evaluation_config import EvaluationConfig


class UniversalWinRateEvaluator:
    """Win Rateè¯„ä¼°å™¨ - æ”¯æŒè‡ªå®šä¹‰ç»´åº¦"""

    def __init__(
        self,
        llm: Optional[HelloAgentsLLM] = None,
        judge_model: str = "gpt-4o",
        eval_config: Optional[EvaluationConfig] = None,
        field_mapping: Optional[Dict[str, str]] = None
    ):
        """
        åˆå§‹åŒ–Win Rateè¯„ä¼°å™¨

        Args:
            llm: LLMå®ä¾‹
            judge_model: è¯„å§”æ¨¡å‹åç§°
            eval_config: è¯„ä¼°é…ç½®ï¼Œæ”¯æŒè‡ªå®šä¹‰ç»´åº¦
            field_mapping: å­—æ®µæ˜ å°„
        """
        self.llm = llm or HelloAgentsLLM(model=judge_model)
        self.judge_model = judge_model
        self.eval_config = eval_config or EvaluationConfig()
        self.field_mapping = field_mapping
        
    def compare_pair(
        self,
        problem_a: Dict[str, Any],
        problem_b: Dict[str, Any],
        label_a: str = "A",
        label_b: str = "B"
    ) -> Dict[str, Any]:
        """å¯¹æ¯”ä¸¤ä¸ªé—®é¢˜ï¼Œåˆ¤æ–­å“ªä¸ªæ›´å¥½"""
        # é€‚é…å­—æ®µ
        problem_a = self._adapt_fields(problem_a)
        problem_b = self._adapt_fields(problem_b)

        start_time = time.time()

        # æ„å»ºå¯¹æ¯”æç¤ºè¯ï¼ˆæ”¯æŒè‡ªå®šä¹‰ç»´åº¦ï¼‰
        prompt = self._build_comparison_prompt(problem_a, problem_b, label_a, label_b)

        messages = [{"role": "user", "content": prompt}]
        response = self.llm.invoke(messages)

        winner, reason = self._parse_comparison_response(response, label_a, label_b)

        execution_time = time.time() - start_time

        return {
            "problem_a_id": problem_a.get("id", "unknown"),
            "problem_b_id": problem_b.get("id", "unknown"),
            "winner": winner,
            "reason": reason,
            "comparison_text": response,
            "execution_time": execution_time
        }
    
    def evaluate_win_rate(
        self,
        generated_problems: List[Dict[str, Any]],
        reference_problems: List[Dict[str, Any]],
        num_comparisons: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°ç”Ÿæˆæ•°æ®ç›¸å¯¹äºå‚è€ƒæ•°æ®çš„èƒœç‡
        
        Args:
            generated_problems: ç”Ÿæˆçš„é—®é¢˜åˆ—è¡¨
            reference_problems: å‚è€ƒé—®é¢˜åˆ—è¡¨ï¼ˆå¦‚AIMEçœŸé¢˜ï¼‰
            num_comparisons: å¯¹æ¯”æ¬¡æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™å¯¹æ¯”æ‰€æœ‰å¯èƒ½çš„é…å¯¹
        
        Returns:
            èƒœç‡è¯„ä¼°ç»“æœ
        """
        print(f"\nğŸ† å¼€å§‹Win Rateè¯„ä¼°")
        print(f"   è¯„å§”æ¨¡å‹: {self.judge_model}")
        print(f"   è¯„ä¼°ç»´åº¦: {', '.join(self.eval_config.get_dimension_names())}")
        print(f"   ç”Ÿæˆæ•°æ®: {len(generated_problems)} ä¸ª")
        print(f"   å‚è€ƒæ•°æ®: {len(reference_problems)} ä¸ª")

        # ç¡®å®šå¯¹æ¯”æ¬¡æ•°
        if num_comparisons is None:
            num_comparisons = min(len(generated_problems), len(reference_problems))

        # é™åˆ¶å¯¹æ¯”æ¬¡æ•°ä¸è¶…è¿‡ç”Ÿæˆé¢˜ç›®æ•°é‡
        num_comparisons = min(num_comparisons, len(generated_problems))

        print(f"   å¯¹æ¯”æ¬¡æ•°: {num_comparisons}")

        gen_indices = random.sample(range(len(generated_problems)), num_comparisons)

        print(f"   é‡‡æ ·æ–¹å¼: éšæœºé‡‡æ ·")

        # è¿›è¡Œæˆå¯¹å¯¹æ¯”
        comparisons = []
        wins = 0
        losses = 0
        ties = 0

        for i, gen_idx in enumerate(gen_indices):
            gen_problem = generated_problems[gen_idx]
            # éšæœºé€‰æ‹©ä¸€ä¸ªå‚è€ƒé¢˜ç›®
            ref_idx = random.randint(0, len(reference_problems) - 1)
            ref_problem = reference_problems[ref_idx]

            print(f"\n   å¯¹æ¯”è¿›åº¦: {i + 1}/{num_comparisons}")
            print(f"   ç”Ÿæˆé¢˜ç›®: #{gen_idx + 1}, å‚è€ƒé¢˜ç›®: #{ref_idx + 1}")

            # éšæœºåŒ–é¢˜ç›®é¡ºåºä»¥é¿å…ä½ç½®åå‘
            if random.random() < 0.5:
                # Generatedåœ¨å‰
                result = self.compare_pair(
                    gen_problem,
                    ref_problem,
                    label_a="Problem A",
                    label_b="Problem B"
                )
                # è®°å½•å®é™…é¡ºåº
                result["actual_order"] = {"A": "Generated", "B": "Reference"}

                # è½¬æ¢winner
                if result["winner"] == "Problem A":
                    actual_winner = "Generated"
                elif result["winner"] == "Problem B":
                    actual_winner = "Reference"
                else:
                    actual_winner = "Tie"
            else:
                # Referenceåœ¨å‰
                result = self.compare_pair(
                    ref_problem,
                    gen_problem,
                    label_a="Problem A",
                    label_b="Problem B"
                )
                # è®°å½•å®é™…é¡ºåº
                result["actual_order"] = {"A": "Reference", "B": "Generated"}

                # è½¬æ¢winner
                if result["winner"] == "Problem A":
                    actual_winner = "Reference"
                elif result["winner"] == "Problem B":
                    actual_winner = "Generated"
                else:
                    actual_winner = "Tie"

            result["actual_winner"] = actual_winner
            comparisons.append(result)

            # ç»Ÿè®¡èƒœè´Ÿ
            if actual_winner == "Generated":
                wins += 1
                print(f"   âœ“ Generatedèƒœå‡º")
            elif actual_winner == "Reference":
                losses += 1
                print(f"   âœ— Referenceèƒœå‡º")
            else:
                ties += 1
                print(f"   = å¹³å±€")
        
        # è®¡ç®—èƒœç‡
        win_rate = wins / num_comparisons if num_comparisons > 0 else 0
        loss_rate = losses / num_comparisons if num_comparisons > 0 else 0
        tie_rate = ties / num_comparisons if num_comparisons > 0 else 0
        
        metrics = {
            "win_rate": win_rate,
            "loss_rate": loss_rate,
            "tie_rate": tie_rate,
            "wins": wins,
            "losses": losses,
            "ties": ties,
            "total_comparisons": num_comparisons
        }
        
        print(f"\nğŸ“Š Win Rateè¯„ä¼°ç»“æœ:")
        print(f"   èƒœç‡: {win_rate:.2%}")
        print(f"   è´¥ç‡: {loss_rate:.2%}")
        print(f"   å¹³å±€ç‡: {tie_rate:.2%}")
        
        return {
            "comparisons": comparisons,
            "metrics": metrics,
            "evaluation_date": datetime.now().isoformat(),
            "judge_model": self.judge_model,
            "dimensions": self.eval_config.get_dimension_names()
        }
    
    def _build_comparison_prompt(
        self,
        problem_a: Dict[str, Any],
        problem_b: Dict[str, Any],
        label_a: str,
        label_b: str
    ) -> str:
        """æ„å»ºå¯¹æ¯”æç¤ºè¯ï¼ˆæ”¯æŒè‡ªå®šä¹‰ç»´åº¦ï¼‰"""
        has_solution_a = bool(problem_a.get('solution', '').strip())
        has_solution_b = bool(problem_b.get('solution', '').strip())

        # æ„å»ºé¢˜ç›®Açš„æ–‡æœ¬
        problem_a_text = f"ã€é¢˜ç›®{label_a}ã€‘\n"
        problem_a_text += f"é—®é¢˜ï¼š{problem_a.get('problem', '')}\n"
        problem_a_text += f"ç­”æ¡ˆï¼š{problem_a.get('answer', '')}"
        if has_solution_a:
            problem_a_text += f"\nè§£ç­”ï¼š{problem_a.get('solution', '')}"

        # æ„å»ºé¢˜ç›®Bçš„æ–‡æœ¬
        problem_b_text = f"ã€é¢˜ç›®{label_b}ã€‘\n"
        problem_b_text += f"é—®é¢˜ï¼š{problem_b.get('problem', '')}\n"
        problem_b_text += f"ç­”æ¡ˆï¼š{problem_b.get('answer', '')}"
        if has_solution_b:
            problem_b_text += f"\nè§£ç­”ï¼š{problem_b.get('solution', '')}"

        # åŠ¨æ€ç”Ÿæˆç»´åº¦è¯´æ˜ï¼ˆæŒ‰è‡ªå®šä¹‰ç»´åº¦é¡ºåºï¼‰
        dimension_list = "\n".join([
            f"{idx}. {dim.name}ï¼š{dim.description}"
            for idx, dim in enumerate(self.eval_config.dimensions, 1)
        ])

        prompt = f"""è¯·æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªå†…å®¹çš„è´¨é‡ï¼Œåˆ¤æ–­å“ªä¸ªæ›´å¥½ã€‚

{problem_a_text}

{problem_b_text}

è¯·ä»ä»¥ä¸‹æ–¹é¢è¿›è¡Œæ¯”è¾ƒï¼š
{dimension_list}

æ¯”è¾ƒè¦æ±‚ï¼š
1. å…¨é¢åˆ†æä¸¤ä¸ªå†…å®¹åœ¨å„ç»´åº¦ä¸Šçš„å·®å¼‚
2. å¯¹æ¯ä¸ªç»´åº¦éƒ½è¦ç»™å‡ºå…·ä½“çš„å¯¹æ¯”æ„è§
3. ç»¼åˆæ‰€æœ‰ç»´åº¦çš„è¡¨ç°åšå‡ºæœ€ç»ˆåˆ¤æ–­
4. åªæœ‰åœ¨ä¸¤ä¸ªå†…å®¹åœ¨æ‰€æœ‰æˆ–å¤§å¤šæ•°ç»´åº¦éƒ½ç›¸å½“æ—¶ï¼Œæ‰é€‰æ‹©"Tie"
5. æä¾›è¯¦ç»†çš„ç†ç”±ï¼Œå…·ä½“è¯´æ˜å·®å¼‚ä¹‹å¤„

è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»æ˜¯ä»¥ä¸‹JSONæ ¼å¼ï¼‰ï¼š
```json
{{
    "winner": "A" æˆ– "B" æˆ– "Tie",
    "reason": "è¯¦ç»†çš„æ¯”è¾ƒåˆ†æï¼Œè¯´æ˜å„ç»´åº¦çš„å·®å¼‚å’Œæœ€ç»ˆåˆ¤æ–­ç†ç”±"
}}
```

é‡è¦ï¼šwinnerå­—æ®µå¿…é¡»æ˜¯ä»¥ä¸‹ä¸‰ä¸ªä¹‹ä¸€ï¼šAã€B æˆ– Tie
"""
        return prompt
    
    def _parse_comparison_response(
        self,
        response: str,
        label_a: str,
        label_b: str
    ) -> Tuple[str, str]:
        """è§£æå¯¹æ¯”å“åº”"""
        try:
            # æå–JSONéƒ¨åˆ†
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
            else:
                json_str = response.strip()

            # ä¿®å¤LaTeXè½¬ä¹‰é—®é¢˜
            import re
            try:
                data = json.loads(json_str)
            except json.JSONDecodeError:
                # ä¿®å¤LaTeXè½¬ä¹‰ï¼šå°† \frac è½¬ä¸º \\frac
                fixed_json_str = re.sub(r'(?<!\\)\\(?!["\\/bfnrtu])', r'\\\\', json_str)
                data = json.loads(fixed_json_str)

            winner = data.get("winner", "").strip()
            reason = data.get("reason", "No reason provided")

            # è§„èŒƒåŒ–winnerå€¼ï¼šå¤„ç† A/B/Tie æ˜ å°„åˆ°å®é™…çš„label
            if winner == "A":
                winner = label_a
            elif winner == "B":
                winner = label_b
            elif winner == "Tie":
                winner = "Tie"
            else:
                # å¦‚æœè¿”å›çš„æ˜¯labelæœ¬èº«ï¼Œä¿ç•™å®ƒ
                if winner not in [label_a, label_b]:
                    # æ— æ•ˆçš„winnerï¼Œé»˜è®¤ä¸ºTie
                    winner = "Tie"

            return winner, reason

        except Exception as e:
            print(f"âš ï¸ è§£æå¯¹æ¯”å“åº”å¤±è´¥: {e}")
            return "Tie", "Failed to parse response"
    
    def _adapt_fields(self, problem: Dict[str, Any]) -> Dict[str, Any]:
        """é€‚é…å­—æ®µ"""
        if not self.field_mapping:
            return problem

        adapted = {}
        for standard_key, source_key in self.field_mapping.items():
            if source_key in problem:
                adapted[standard_key] = problem[source_key]

        for key, value in problem.items():
            if key not in self.field_mapping.values():
                adapted[key] = value

        for key in ["id", "problem", "answer"]:
            if key not in adapted and key in problem:
                adapted[key] = problem[key]

        return adapted

    def export_results(
        self,
        results: Dict[str, Any],
        output_path: str
    ):
        """å¯¼å‡ºè¯„ä¼°ç»“æœ"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… Win Rateç»“æœå·²ä¿å­˜: {output_path}")

