"""RAGå·¥å…·

ä¸ºHelloAgentsæ¡†æ¶æä¾›æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)èƒ½åŠ›çš„å·¥å…·å®ç°ã€‚
å¯ä»¥ä½œä¸ºå·¥å…·æ·»åŠ åˆ°ä»»ä½•Agentä¸­ï¼Œè®©Agentå…·å¤‡çŸ¥è¯†åº“æ£€ç´¢åŠŸèƒ½ã€‚
"""

from typing import Dict, Any, List
import os

from ..base import Tool, ToolParameter
from ...memory.rag import (
    SentenceTransformerEmbedding, TFIDFEmbedding, HuggingFaceEmbedding,
    DocumentProcessor, Document,
    VectorRetriever, HybridRetriever,
    create_embedding_model_with_fallback
)
from ...memory.storage import ChromaVectorStore

class RAGTool(Tool):
    """RAGå·¥å…·
    
    ä¸ºAgentæä¾›çŸ¥è¯†åº“æ£€ç´¢åŠŸèƒ½ï¼š
    - æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
    - æ£€ç´¢ç›¸å…³æ–‡æ¡£
    - ç®¡ç†çŸ¥è¯†åº“
    - æ”¯æŒå¤šç§æ£€ç´¢ç­–ç•¥
    """
    
    def __init__(
        self,
        knowledge_base_path: str = "./knowledge_base",
        embedding_model: str = "sentence-transformers",
        retrieval_strategy: str = "vector"
    ):
        super().__init__(
            name="rag",
            description="RAGå·¥å…· - å¯ä»¥ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯æ¥å¢å¼ºå›ç­”"
        )
        
        self.knowledge_base_path = knowledge_base_path
        self.embedding_model_name = embedding_model
        self.retrieval_strategy = retrieval_strategy
        
        # ç¡®ä¿çŸ¥è¯†åº“ç›®å½•å­˜åœ¨
        os.makedirs(knowledge_base_path, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components()
    
    def _init_components(self):
        """åˆå§‹åŒ–RAGç»„ä»¶"""
        try:
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ - ä½¿ç”¨æ™ºèƒ½fallback
            if self.embedding_model_name == "sentence-transformers":
                self.embedding_model = create_embedding_model_with_fallback("sentence_transformer")
            elif self.embedding_model_name == "huggingface":
                self.embedding_model = create_embedding_model_with_fallback("huggingface")
            elif self.embedding_model_name == "tfidf":
                self.embedding_model = TFIDFEmbedding()
            else:
                # é»˜è®¤ä½¿ç”¨æ™ºèƒ½fallback
                self.embedding_model = create_embedding_model_with_fallback("sentence_transformer")
            
            # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
            self.document_processor = DocumentProcessor()
            
            # åˆå§‹åŒ–å‘é‡å­˜å‚¨
            from ...memory.storage import ChromaVectorStore
            self.vector_store = ChromaVectorStore(
                collection_name="rag_knowledge_base",
                persist_directory=os.path.join(self.knowledge_base_path, "chroma_db")
            )

            # åˆå§‹åŒ–æ£€ç´¢å™¨
            if self.retrieval_strategy == "vector":
                self.retriever = VectorRetriever(
                    embedding_model=self.embedding_model,
                    vector_store=self.vector_store
                )
            else:
                self.retriever = HybridRetriever(
                    vector_retriever=VectorRetriever(
                        embedding_model=self.embedding_model,
                        vector_store=self.vector_store
                    ),
                    keyword_retriever=None  # å¯ä»¥åç»­æ·»åŠ 
                )
            
            self.initialized = True
            
        except Exception as e:
            self.initialized = False
            self.init_error = str(e)

    def run(self, parameters: Dict[str, Any]) -> str:
        """æ‰§è¡Œå·¥å…· - ToolåŸºç±»è¦æ±‚çš„æ¥å£

        Args:
            parameters: å·¥å…·å‚æ•°å­—å…¸ï¼Œå¿…é¡»åŒ…å«actionå‚æ•°

        Returns:
            æ‰§è¡Œç»“æœå­—ç¬¦ä¸²
        """
        if not self.validate_parameters(parameters):
            return "âŒ å‚æ•°éªŒè¯å¤±è´¥ï¼šç¼ºå°‘å¿…éœ€çš„å‚æ•°"

        action = parameters.get("action")
        # ç§»é™¤actionå‚æ•°ï¼Œä¼ é€’å…¶ä½™å‚æ•°ç»™executeæ–¹æ³•
        kwargs = {k: v for k, v in parameters.items() if k != "action"}

        return self.execute(action, **kwargs)

    def get_parameters(self) -> List[ToolParameter]:
        """è·å–å·¥å…·å‚æ•°å®šä¹‰ - ToolåŸºç±»è¦æ±‚çš„æ¥å£"""
        return [
            ToolParameter(
                name="action",
                type="string",
                description="è¦æ‰§è¡Œçš„æ“ä½œï¼šadd_document(æ·»åŠ æ–‡æ¡£), add_text(æ·»åŠ æ–‡æœ¬), search(æœç´¢), list_documents(åˆ—å‡ºæ–‡æ¡£), stats(è·å–ç»Ÿè®¡), clear(æ¸…ç©ºçŸ¥è¯†åº“)",
                required=True
            ),
            ToolParameter(
                name="file_path",
                type="string",
                description="æ–‡æ¡£æ–‡ä»¶è·¯å¾„ï¼ˆadd_documentæ“ä½œæ—¶å¿…éœ€ï¼‰",
                required=False
            ),
            ToolParameter(
                name="text",
                type="string",
                description="è¦æ·»åŠ çš„æ–‡æœ¬å†…å®¹ï¼ˆadd_textæ“ä½œæ—¶å¿…éœ€ï¼‰",
                required=False
            ),
            ToolParameter(
                name="document_id",
                type="string",
                description="æ–‡æ¡£IDï¼ˆå¯é€‰ï¼Œç”¨äºæ ‡è¯†æ–‡æ¡£ï¼‰",
                required=False
            ),
            ToolParameter(
                name="query",
                type="string",
                description="æœç´¢æŸ¥è¯¢ï¼ˆsearchæ“ä½œæ—¶å¿…éœ€ï¼‰",
                required=False
            ),
            ToolParameter(
                name="limit",
                type="integer",
                description="æœç´¢ç»“æœæ•°é‡é™åˆ¶ï¼ˆé»˜è®¤ï¼š5ï¼‰",
                required=False,
                default=5
            ),
            ToolParameter(
                name="min_score",
                type="number",
                description="æœ€å°ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆé»˜è®¤ï¼š0.1ï¼‰",
                required=False,
                default=0.1
            )
        ]
    
    def execute(self, action: str, **kwargs) -> str:
        """æ‰§è¡ŒRAGæ“ä½œ
        
        æ”¯æŒçš„æ“ä½œï¼š
        - add_document: æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
        - add_text: æ·»åŠ æ–‡æœ¬åˆ°çŸ¥è¯†åº“
        - search: æœç´¢çŸ¥è¯†åº“
        - list_documents: åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£
        - stats: è·å–çŸ¥è¯†åº“ç»Ÿè®¡
        """
        
        if not self.initialized:
            return f"âŒ RAGå·¥å…·åˆå§‹åŒ–å¤±è´¥: {getattr(self, 'init_error', 'æœªçŸ¥é”™è¯¯')}"
        
        if action == "add_document":
            return self._add_document(**kwargs)
        elif action == "add_text":
            return self._add_text(**kwargs)
        elif action == "search":
            return self._search(**kwargs)
        elif action == "list_documents":
            return self._list_documents()
        elif action == "stats":
            return self._get_stats()
        elif action == "clear":
            return self.clear_knowledge_base()
        else:
            return f"ä¸æ”¯æŒçš„æ“ä½œ: {action}ã€‚æ”¯æŒçš„æ“ä½œ: add_document, add_text, search, list_documents, stats, clear"

    def _add_document(self, file_path: str, document_id: str = None) -> str:
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        try:
            if not os.path.exists(file_path):
                return f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            
            # è¯»å–æ–‡æ¡£å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # å¤„ç†æ–‡æ¡£
            document_id = document_id or os.path.basename(file_path)
            document = Document(content=content, metadata={"source": file_path}, doc_id=document_id)
            chunks = self.document_processor.process_document(document)
            
            # æ·»åŠ åˆ°çŸ¥è¯†åº“
            chunk_ids = self.retriever.add_documents(chunks)
            added_count = len(chunk_ids)
            
            return f"âœ… æ–‡æ¡£å·²æ·»åŠ åˆ°çŸ¥è¯†åº“: {document_id} ({added_count} ä¸ªç‰‡æ®µ)"
            
        except Exception as e:
            return f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {str(e)}"
    
    def _add_text(self, text: str, document_id: str = None, metadata: Dict[str, Any] = None) -> str:
        """æ·»åŠ æ–‡æœ¬åˆ°çŸ¥è¯†åº“"""
        try:
            # å¤„ç†æ–‡æœ¬
            document_id = document_id or f"text_{hash(text) % 10000}"
            document = Document(content=text, metadata=metadata or {}, doc_id=document_id)
            chunks = self.document_processor.process_document(document)
            
            # æ·»åŠ åˆ°çŸ¥è¯†åº“
            chunk_ids = self.retriever.add_documents(chunks)
            added_count = len(chunk_ids)
            
            return f"âœ… æ–‡æœ¬å·²æ·»åŠ åˆ°çŸ¥è¯†åº“: {document_id} ({added_count} ä¸ªç‰‡æ®µ)"
            
        except Exception as e:
            return f"âŒ æ·»åŠ æ–‡æœ¬å¤±è´¥: {str(e)}"
    
    def _search(self, query: str, limit: int = 5, min_score: float = 0.1) -> str:
        """æœç´¢çŸ¥è¯†åº“"""
        try:
            results = self.retriever.retrieve(query, top_k=limit)
            
            if not results:
                return f"ğŸ” æœªæ‰¾åˆ°ä¸ '{query}' ç›¸å…³çš„å†…å®¹"
            
            # è¿‡æ»¤ä½åˆ†ç»“æœ
            filtered_results = [r for r in results if r.metadata.get('similarity_score', 0) >= min_score]

            if not filtered_results:
                return f"ğŸ” æœªæ‰¾åˆ°è¶³å¤Ÿç›¸å…³çš„å†…å®¹ (æœ€ä½åˆ†æ•°è¦æ±‚: {min_score})"

            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            formatted_results.append(f"ğŸ” æ‰¾åˆ° {len(filtered_results)} æ¡ç›¸å…³å†…å®¹:")

            for i, doc_chunk in enumerate(filtered_results, 1):
                score = doc_chunk.metadata.get('similarity_score', 0)

                content_preview = doc_chunk.content[:100] + "..." if len(doc_chunk.content) > 100 else doc_chunk.content
                source_info = f"æ¥æº: {doc_chunk.doc_id}" if doc_chunk.doc_id else ""

                formatted_results.append(
                    f"{i}. {content_preview} (ç›¸å…³æ€§: {score:.2f}) {source_info}"
                )
            
            return "\n".join(formatted_results)
            
        except Exception as e:
            return f"âŒ æœç´¢å¤±è´¥: {str(e)}"
    
    def _list_documents(self) -> str:
        """åˆ—å‡ºçŸ¥è¯†åº“çš„æ–‡æ¡£ç‰‡æ®µæ•°é‡ï¼ˆåŸºäºå‘é‡å­˜å‚¨ç»Ÿè®¡ï¼‰"""
        try:
            vector_stats = self.vector_store.get_collection_stats()
            # å…¼å®¹ä¸åŒåç«¯çš„å­—æ®µå‘½å
            total = (
                vector_stats.get("total_documents")
                or vector_stats.get("total_entities")
                or vector_stats.get("count")
                or 0
            )
            store_type = vector_stats.get("store_type", "unknown")
            return f"ğŸ“š çŸ¥è¯†åº“ï¼ˆ{store_type}ï¼‰åŒ…å« {int(total)} ä¸ªæ–‡æ¡£ç‰‡æ®µ"
        except Exception as e:
            return f"âŒ è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}"

    def _get_stats(self) -> str:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡"""
        try:
            stats_info = [
                f"ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡",
                f"å­˜å‚¨æ ¹è·¯å¾„: {self.knowledge_base_path}",
                f"åµŒå…¥æ¨¡å‹: {self.embedding_model_name}",
                f"æ£€ç´¢ç­–ç•¥: {self.retrieval_strategy}"
            ]

            # è·å–å‘é‡å­˜å‚¨ç»Ÿè®¡ï¼ˆå…¼å®¹ä¸åŒå®ç°ï¼‰
            try:
                vector_stats = self.vector_store.get_collection_stats()
                store_type = vector_stats.get("store_type", "unknown")
                total = (
                    vector_stats.get("total_documents")
                    or vector_stats.get("total_entities")
                    or vector_stats.get("count")
                    or 0
                )
                stats_info.append(f"å­˜å‚¨åç«¯: {store_type}")
                stats_info.append(f"æ–‡æ¡£ç‰‡æ®µæ•°: {int(total)}")

                # å…¶ä»–å¸¸è§å­—æ®µï¼ˆæŒ‰å­˜åœ¨æ€§è¿½åŠ ï¼‰
                for k in [
                    "vector_dimension", "dimension",
                    "collection_name", "persist_directory",
                    "index_type", "persist_path"
                ]:
                    if k in vector_stats:
                        stats_info.append(f"{k}: {vector_stats[k]}")
            except Exception:
                stats_info.append("å­˜å‚¨ç»Ÿè®¡è¯»å–å¤±è´¥ï¼Œå¯èƒ½æœªåˆå§‹åŒ–æˆ–æ— å¯ç”¨åç«¯")

            return "\n".join(stats_info)

        except Exception as e:
            return f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}"

    def get_relevant_context(self, query: str, limit: int = 3) -> str:
        """ä¸ºæŸ¥è¯¢è·å–ç›¸å…³ä¸Šä¸‹æ–‡
        
        è¿™ä¸ªæ–¹æ³•å¯ä»¥è¢«Agentè°ƒç”¨æ¥è·å–ç›¸å…³çš„çŸ¥è¯†åº“ä¸Šä¸‹æ–‡
        """
        try:
            results = self.retriever.retrieve(query, top_k=limit)
            
            if not results:
                return ""
            
            context_parts = ["ç›¸å…³çŸ¥è¯†:"]
            for result in results:
                doc = result['document']
                context_parts.append(f"- {doc.content}")
            
            return "\n".join(context_parts)
            
        except Exception as e:
            return f"è·å–ä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}"
    
    def batch_add_texts(self, texts: List[str], document_ids: List[str] = None) -> str:
        """æ‰¹é‡æ·»åŠ æ–‡æœ¬"""
        try:
            if document_ids and len(document_ids) != len(texts):
                return "âŒ æ–‡æœ¬æ•°é‡å’Œæ–‡æ¡£IDæ•°é‡ä¸åŒ¹é…"
            
            total_chunks = 0
            for i, text in enumerate(texts):
                doc_id = document_ids[i] if document_ids else f"batch_text_{i}"
                document = Document(content=text, metadata={}, doc_id=doc_id)
                chunks = self.document_processor.process_document(document)
                
                chunk_ids = self.retriever.add_documents(chunks)
                total_chunks += len(chunk_ids)
            
            return f"âœ… æ‰¹é‡æ·»åŠ å®Œæˆ: {len(texts)} ä¸ªæ–‡æœ¬, {total_chunks} ä¸ªç‰‡æ®µ"
            
        except Exception as e:
            return f"âŒ æ‰¹é‡æ·»åŠ å¤±è´¥: {str(e)}"
    
    def clear_knowledge_base(self) -> str:
        """æ¸…ç©ºçŸ¥è¯†åº“ï¼ˆåˆ é™¤æŒä¹…åŒ–æ•°æ®å¹¶é‡å»ºï¼‰"""
        import shutil
        try:
            # å°è¯•åˆ é™¤å‘é‡å­˜å‚¨çš„æŒä¹…åŒ–ç›®å½•
            persist_dir = getattr(self.vector_store, "persist_directory", None) or getattr(self.vector_store, "persist_path", None)
            if persist_dir and os.path.exists(persist_dir):
                shutil.rmtree(persist_dir, ignore_errors=True)
            # é‡æ–°åˆå§‹åŒ–ç»„ä»¶
            self._init_components()
            return "âœ… çŸ¥è¯†åº“å·²æ¸…ç©º"
        except Exception as e:
            return f"âŒ æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}"
