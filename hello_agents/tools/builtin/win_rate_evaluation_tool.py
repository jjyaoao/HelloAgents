"""
Win Rate è¯„ä¼°å·¥å…·

ä½¿ç”¨æ–¹å¼ï¼š
    from hello_agents.tools.builtin import WinRateEvaluationTool
    from hello_agents.evaluation.benchmarks.data_generation_Universal import UniversalDataset

    # 1. åŠ è½½æ•°æ®é›†ï¼ˆjsonä¸ºä¾‹ï¼‰
    # æœ¬åœ°æ•°æ®
    generated_dataset = UniversalDataset(
        source_type="local",
        source_config={"path": "generated.json"}
    )

    # 2. åˆ›å»ºè¯„ä¼°å™¨
    evaluator = WinRateEvaluationTool(template="math")

    # 3. è¿è¡Œè¯„ä¼°
    results = evaluator.evaluate(generated, reference, num_comparisons=10)

    # 4. å¯¼å‡ºç»“æœ
    evaluator.export_report(results, output_dir="results")
"""

import json
from pathlib import Path
from typing import List, Dict, Tuple, Any, Optional
from datetime import datetime

from hello_agents.evaluation.benchmarks.data_generation_Universal import (
    EvaluationConfig,
    UniversalWinRateEvaluator as BaseWinRateEvaluator,
    UniversalDataset,
)
from hello_agents.core.llm import HelloAgentsLLM


# ä¿ç•™ UniversalWinRateDataset ç”¨äºå‘åå…¼å®¹ï¼ˆå†…éƒ¨ä½¿ç”¨ UniversalDatasetï¼‰
class UniversalWinRateDataset:
    """Win Rate æ•°æ®é›†åŠ è½½å™¨ï¼ˆæ¨èä½¿ç”¨ UniversalDatasetï¼‰

    æ­¤ç±»ä»…ä¿ç•™ä»¥ä¿è¯å‘åå…¼å®¹æ€§ï¼Œå†…éƒ¨å·²è½¬æ¢ä¸ºä½¿ç”¨ UniversalDatasetã€‚
    æ–°ä»£ç åº”ç›´æ¥ä½¿ç”¨ UniversalDatasetã€‚
    """

    def __init__(
        self,
        generated_path: str,
        reference_path: Optional[str] = None,
        format: str = "custom",
        field_mapping: Optional[Dict[str, str]] = None
    ):
        """
        åˆå§‹åŒ–æ•°æ®é›†åŠ è½½å™¨

        Args:
            generated_path: ç”Ÿæˆæ•°æ®æ–‡ä»¶è·¯å¾„
            reference_path: å‚è€ƒæ•°æ®æ–‡ä»¶è·¯å¾„ (ä¸ºNoneæ—¶ä½¿ç”¨ç”Ÿæˆæ•°æ®åä¸€åŠ)
            format: æ•°æ®æ ¼å¼ï¼ˆå·²å¿½ç•¥ï¼Œæ”¹ä¸ºä½¿ç”¨ UniversalDataset çš„è‡ªåŠ¨æ£€æµ‹ï¼‰
            field_mapping: å­—æ®µæ˜ å°„ {æ ‡å‡†å­—æ®µ -> æºå­—æ®µ}

        Deprecated:
            ä½¿ç”¨ UniversalDataset æ›¿ä»£ï¼š
            generated_dataset = UniversalDataset(
                source_type="local",
                source_config={"path": generated_path},
                field_mapping=field_mapping
            )
            reference_dataset = UniversalDataset(
                source_type="local",
                source_config={"path": reference_path},
                field_mapping=field_mapping
            )
        """
        # ä½¿ç”¨ UniversalDataset æ›¿ä»£åŸå®ç°
        self._generated_dataset = UniversalDataset(
            source_type="local",
            source_config={"path": generated_path},
            field_mapping=field_mapping
        )
        self._reference_path = reference_path
        self._field_mapping = field_mapping
        self.generated_data = None
        self.reference_data = None

    def load(self) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        åŠ è½½æ•°æ®

        Returns:
            (ç”Ÿæˆæ•°æ®åˆ—è¡¨, å‚è€ƒæ•°æ®åˆ—è¡¨)
        """
        # åŠ è½½ç”Ÿæˆæ•°æ®
        self.generated_data = self._generated_dataset.load()

        # åŠ è½½å‚è€ƒæ•°æ®
        if self._reference_path:
            reference_dataset = UniversalDataset(
                source_type="local",
                source_config={"path": self._reference_path},
                field_mapping=self._field_mapping
            )
            self.reference_data = reference_dataset.load()
        else:
            # ä½¿ç”¨ç”Ÿæˆæ•°æ®çš„åä¸€åŠä½œä¸ºå‚è€ƒ
            mid = len(self.generated_data) // 2
            self.reference_data = self.generated_data[mid:]
            self.generated_data = self.generated_data[:mid]

        return self.generated_data, self.reference_data

    def __len__(self) -> Tuple[int, int]:
        """è·å–æ•°æ®é›†å¤§å°"""
        if self.generated_data is None or self.reference_data is None:
            self.load()
        return len(self.generated_data), len(self.reference_data)


class UniversalWinRateTool:
    """Win Rate è¯„ä¼°å·¥å…· - é€šç”¨ç‰ˆæœ¬

    æ”¯æŒå¤šç§æ•°æ®æºå’Œè¯„ä¼°æ¨¡æ¿çš„èƒœç‡å¯¹æ¯”å·¥å…·ã€‚
    """

    def __init__(
        self,
        template: str = "math",
        llm: HelloAgentsLLM = None,
        field_mapping: Optional[Dict[str, str]] = None
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å·¥å…·

        Args:
            template: è¯„ä¼°æ¨¡æ¿ ("math" / "code" / "writing" / "qa")
            llm: LLMå®ä¾‹ï¼ˆå¿…éœ€ï¼Œåº”ç”±è°ƒç”¨è€…åˆ›å»ºï¼‰
            field_mapping: å­—æ®µæ˜ å°„
        """
        if llm is None:
            raise ValueError("llmå‚æ•°ä¸èƒ½ä¸ºç©ºï¼Œè¯·ä¼ å…¥HelloAgentsLLMå®ä¾‹")
        self.template = template
        self.llm = llm
        self.field_mapping = field_mapping or {"problem": "problem", "answer": "answer"}
        self.eval_config = EvaluationConfig.load_template(template)
        self.results = None

    def evaluate(
        self,
        generated: List[Dict[str, Any]],
        reference: List[Dict[str, Any]],
        num_comparisons: int = 0
    ) -> Dict[str, Any]:
        """
        è¿è¡Œè¯„ä¼°

        Args:
            generated: ç”Ÿæˆæ•°æ®åˆ—è¡¨
            reference: å‚è€ƒæ•°æ®åˆ—è¡¨
            num_comparisons: å¯¹æ¯”æ¬¡æ•° (0=min(len(generated), len(reference)))

        Returns:
            è¯„ä¼°ç»“æœ
        """
        # ç¡®å®šå¯¹æ¯”æ¬¡æ•°
        if num_comparisons <= 0:
            num_comparisons = min(len(generated), len(reference))

        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = BaseWinRateEvaluator(
            llm=self.llm,
            eval_config=self.eval_config,
            field_mapping=self.field_mapping
        )

        # è¿è¡Œè¯„ä¼°
        self.results = evaluator.evaluate_win_rate(
            generated_problems=generated,
            reference_problems=reference,
            num_comparisons=num_comparisons
        )

        return self.results

    def run(self, params: Dict[str, Any]) -> str:
        """
        è¿è¡Œå®Œæ•´çš„è¯„ä¼°å·¥ä½œæµï¼ˆé«˜çº§æ¥å£ï¼‰

        Args:
            params: å‚æ•°å­—å…¸ï¼ŒåŒ…å«ï¼š
                - generated_config: ç”Ÿæˆæ•°æ®æºé…ç½® (å¿…éœ€)
                  {"path": "path/to/generated.json"}
                - reference_config: å‚è€ƒæ•°æ®æºé…ç½® (å¿…éœ€)
                  {"path": "path/to/reference.json"}
                - generated_field_mapping: ç”Ÿæˆæ•°æ®å­—æ®µæ˜ å°„ (å¯é€‰)
                  {"problem": "question", "answer": "answer"}
                - reference_field_mapping: å‚è€ƒæ•°æ®å­—æ®µæ˜ å°„ (å¯é€‰)
                - template: è¯„ä¼°æ¨¡æ¿ (å¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–æ—¶çš„æ¨¡æ¿)
                - num_comparisons: å¯¹æ¯”æ¬¡æ•° (å¯é€‰ï¼Œé»˜è®¤: min(len(generated), len(reference)))
                - output_dir: è¾“å‡ºç›®å½• (å¯é€‰ï¼Œé»˜è®¤: "evaluation_results")
                - judge_model: è¯„ä¼°æ¨¡å‹ (å¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–æ—¶çš„æ¨¡å‹)

        Returns:
            åŒ…å«è¯„ä¼°ç»“æœçš„ JSON å­—ç¬¦ä¸²
        """
        import os
        from datetime import datetime

        # è§£æå‚æ•°
        generated_config = params.get("generated_config")
        reference_config = params.get("reference_config")
        generated_field_mapping = params.get("generated_field_mapping", self.field_mapping)
        reference_field_mapping = params.get("reference_field_mapping", {})

        template = params.get("template", self.template)
        num_comparisons = params.get("num_comparisons", 0)
        output_dir = params.get("output_dir", "evaluation_results")

        # éªŒè¯å¿…éœ€å‚æ•°
        if not generated_config or "path" not in generated_config:
            raise ValueError("generated_config must contain 'path' key")
        if not reference_config or "path" not in reference_config:
            raise ValueError("reference_config must contain 'path' key")

        # ä¸ºè¾“å‡ºç›®å½•æ·»åŠ æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(output_dir, f"win_rate_{timestamp}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        print("\n" + "="*70)
        print("ğŸ¯ Win Rate è¯„ä¼°")
        print("="*70)

        # æ­¥éª¤ 1: åŠ è½½ç”Ÿæˆæ•°æ®
        print(f"\nğŸ“¥ æ­¥éª¤ 1: åŠ è½½ç”Ÿæˆæ•°æ®")
        print(f"   æ¥æº: {generated_config['path']}")
        generated_dataset = UniversalDataset(
            source_config=generated_config,
            field_mapping=generated_field_mapping
        )
        generated = generated_dataset.load()
        print(f"   âœ“ å·²åŠ è½½ {len(generated)} æ¡ç”Ÿæˆæ•°æ®")

        # æ­¥éª¤ 2: åŠ è½½å‚è€ƒæ•°æ®
        print(f"\nğŸ“¥ æ­¥éª¤ 2: åŠ è½½å‚è€ƒæ•°æ®")
        print(f"   æ¥æº: {reference_config['path']}")
        reference_dataset = UniversalDataset(
            source_config=reference_config,
            field_mapping=reference_field_mapping
        )
        reference = reference_dataset.load()
        print(f"   âœ“ å·²åŠ è½½ {len(reference)} æ¡å‚è€ƒæ•°æ®")

        # æ­¥éª¤ 3: é…ç½®è¯„ä¼°å™¨
        print(f"\nâš™ï¸  æ­¥éª¤ 3: é…ç½®è¯„ä¼°å™¨")
        self.template = template
        self.field_mapping = generated_field_mapping
        self.eval_config = EvaluationConfig.load_template(template)
        print(f"   æ¨¡æ¿: {template}")
        print(f"   è¯„ä¼°ç»´åº¦: {', '.join(self.eval_config.get_dimension_names())}")

        # æ­¥éª¤ 4: è¿è¡Œè¯„ä¼°
        print(f"\nğŸš€ æ­¥éª¤ 4: å¼€å§‹ Win Rate å¯¹æ¯”è¯„ä¼°")
        results = self.evaluate(generated, reference, num_comparisons=num_comparisons)
        print(f"   âœ“ è¯„ä¼°å®Œæˆ")

        # æ­¥éª¤ 5: å¯¼å‡ºæŠ¥å‘Š
        print(f"\nğŸ’¾ æ­¥éª¤ 5: å¯¼å‡ºè¯„ä¼°æŠ¥å‘Š")
        report_path = self.export_report(results, output_dir=output_dir)
        print(f"   âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        print("\n" + "="*70)
        print("âœ… Win Rate è¯„ä¼°å®Œæˆ")
        print("="*70)

        # è¿”å›ç»“æœ
        metrics = results.get("metrics", {})
        return json.dumps({
            "status": "success",
            "template": template,
            "judge_model": self.llm.model,
            "num_generated": len(generated),
            "num_reference": len(reference),
            "num_comparisons": metrics.get("total_comparisons", 0),
            "win_rate": metrics.get("win_rate", 0),
            "loss_rate": metrics.get("loss_rate", 0),
            "tie_rate": metrics.get("tie_rate", 0),
            "output_dir": output_dir
        }, ensure_ascii=False, indent=2)

    def export_report(self, results: Optional[Dict[str, Any]] = None, output_dir: str = "evaluation_results") -> str:
        """
        å¯¼å‡ºè¯„ä¼°æŠ¥å‘Š

        Args:
            results: è¯„ä¼°ç»“æœ (ä½¿ç”¨æœ€åä¸€æ¬¡evaluateçš„ç»“æœ)
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        if results is None:
            results = self.results

        if results is None:
            raise ValueError("æ²¡æœ‰è¯„ä¼°ç»“æœï¼Œè¯·å…ˆè¿è¡Œ evaluate() æ–¹æ³•")

        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        metrics = results.get("metrics", {})
        comparisons = results.get("comparisons", [])

        # è·å–ç»Ÿè®¡æ•°æ®
        win_rate = metrics.get('win_rate', 0)
        loss_rate = metrics.get('loss_rate', 0)
        tie_rate = metrics.get('tie_rate', 0)
        total_comparisons = metrics.get('total_comparisons', 0)
        wins = metrics.get('wins', 0)
        losses = metrics.get('losses', 0)
        ties = metrics.get('ties', 0)

        # ç”Ÿæˆè´¨é‡è¯„çº§
        win_rate_pct = win_rate * 100
        if win_rate_pct >= 50:
            quality_level = "ä¼˜ç§€"
            quality_icon = "â­â­â­â­â­"
            quality_desc = "ç”Ÿæˆæ•°æ®è´¨é‡ä¼˜äºå‚è€ƒæ•°æ®ï¼ˆèƒœç‡â‰¥50%ï¼‰ã€‚"
        elif win_rate_pct >= 40:
            quality_level = "è‰¯å¥½"
            quality_icon = "â­â­â­â­"
            quality_desc = "ç”Ÿæˆæ•°æ®è´¨é‡æ¥è¿‘å‚è€ƒæ•°æ®ï¼ˆèƒœç‡40%-50%ï¼‰ã€‚"
        elif win_rate_pct >= 30:
            quality_level = "ä¸­ç­‰"
            quality_icon = "â­â­â­"
            quality_desc = "ç”Ÿæˆæ•°æ®è´¨é‡ä½äºå‚è€ƒæ•°æ®ï¼Œä½†å·®è·ä¸å¤§ï¼ˆèƒœç‡30%-40%ï¼‰ã€‚"
        elif win_rate_pct >= 20:
            quality_level = "åŠæ ¼"
            quality_icon = "â­â­"
            quality_desc = "ç”Ÿæˆæ•°æ®è´¨é‡æ˜æ˜¾ä½äºå‚è€ƒæ•°æ®ï¼ˆèƒœç‡20%-30%ï¼‰ã€‚"
        else:
            quality_level = "å¾…æ”¹è¿›"
            quality_icon = "â­"
            quality_desc = "ç”Ÿæˆæ•°æ®è´¨é‡è¿œä½äºå‚è€ƒæ•°æ®ï¼ˆèƒœç‡<20%ï¼‰ã€‚"

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report_lines = [
            "# Win Rate è¯„ä¼°æŠ¥å‘Š\n\n",
            f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
            f"**è¯„ä¼°æ¨¡æ¿**: {self.template}\n",
            f"**å¯¹æ¯”æ¬¡æ•°**: {total_comparisons}\n\n",
            "## èƒœç‡ç»Ÿè®¡\n\n",
            "| æŒ‡æ ‡ | æ•°å€¼ | ç™¾åˆ†æ¯” |\n",
            "|------|------|--------|\n",
            f"| ç”Ÿæˆæ•°æ®èƒœå‡º | {wins}æ¬¡ | {win_rate:.1%} |\n",
            f"| å‚è€ƒæ•°æ®èƒœå‡º | {losses}æ¬¡ | {loss_rate:.1%} |\n",
            f"| å¹³å±€ | {ties}æ¬¡ | {tie_rate:.1%} |\n\n",
            f"<strong>Win Rate</strong>: {win_rate:.1%}\n\n",
            f"{'âœ…' if quality_level in ['ä¼˜ç§€', 'è‰¯å¥½'] else 'âš ï¸'} <strong>{quality_level}</strong>: {quality_desc}\n\n",
            "## å¯¹æ¯”è¯¦æƒ…\n\n",
        ]

        for i, comp in enumerate(comparisons, 1):
            report_lines.append(f"### å¯¹æ¯” {i}\n\n")
            report_lines.append(f"- **èµ¢å®¶**: {comp.get('actual_winner', comp.get('winner', 'N/A'))}\n")
            report_lines.append(f"- **åŸå› **: {comp.get('reason', 'N/A')}\n\n")

        # ä¿å­˜MarkdownæŠ¥å‘Š
        report_path = output_path / "win_rate_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.writelines(report_lines)

        # ä¿å­˜JSONç»“æœ
        results_file = output_path / "win_rate_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        return str(report_path)

    def get_available_templates(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨æ¨¡æ¿"""
        return EvaluationConfig.get_available_templates()
