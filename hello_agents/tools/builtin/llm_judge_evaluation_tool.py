"""
LLM Judge è¯„ä¼°å·¥å…·

ä½¿ç”¨æ–¹å¼ï¼š
    from hello_agents.tools.builtin import LLMJudgeEvaluationTool
    from hello_agents.evaluation.benchmarks.data_generation_Universal import UniversalDataset

    # 1. åŠ è½½æ•°æ®é›†ï¼ˆjsonä¸ºä¾‹ï¼‰
    # æœ¬åœ°æ•°æ®
    dataset = UniversalDataset(
        source_type="local",
        source_config={"path": "data.json"}
    )

    # 2. åˆ›å»ºè¯„ä¼°å™¨
    evaluator = LLMJudgeEvaluationTool(template="math")

    # 3. è¿è¡Œè¯„ä¼°
    results = evaluator.evaluate(data, max_samples=10)

    # 4. å¯¼å‡ºç»“æœ
    evaluator.export_report(results, output_dir="results")
"""

import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

from hello_agents.evaluation.benchmarks.data_generation_Universal import (
    EvaluationConfig,
    LLMJudgeEvaluator as BaseLLMJudgeEvaluator,
    UniversalDataset,
)
from hello_agents.core.llm import HelloAgentsLLM


# ä¿ç•™ LLMJudgeDataset ç”¨äºå‘åå…¼å®¹ï¼ˆå·²è¿‡æ—¶ï¼Œå†…éƒ¨ä½¿ç”¨ UniversalDatasetï¼‰
class LLMJudgeDataset:
    """LLM Judge æ•°æ®é›†åŠ è½½å™¨ï¼ˆå·²è¿‡æ—¶ï¼Œæ¨èä½¿ç”¨ UniversalDatasetï¼‰

    æ­¤ç±»ä»…ä¿ç•™ä»¥ä¿è¯å‘åå…¼å®¹æ€§ï¼Œå†…éƒ¨å·²è½¬æ¢ä¸ºä½¿ç”¨ UniversalDatasetã€‚
    æ–°ä»£ç åº”ç›´æ¥ä½¿ç”¨ UniversalDatasetã€‚
    """

    def __init__(
        self,
        data_path: str,
        format: str = "auto",
        field_mapping: Optional[Dict[str, str]] = None
    ):
        """
        åˆå§‹åŒ–æ•°æ®é›†åŠ è½½å™¨

        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            format: æ•°æ®æ ¼å¼ ("json", "csv" æˆ– "auto")
            field_mapping: å­—æ®µæ˜ å°„ {æ ‡å‡†å­—æ®µ -> æºå­—æ®µ}

        Deprecated:
            ä½¿ç”¨ UniversalDataset æ›¿ä»£ï¼š
            dataset = UniversalDataset(
                source_config={"path": data_path, "format": "auto"},
                field_mapping=field_mapping
            )
        """
        # ä½¿ç”¨ UniversalDataset æ›¿ä»£åŸå®ç°
        self._universal_dataset = UniversalDataset(
            source_config={"path": data_path, "format": format},
            field_mapping=field_mapping
        )
        self._data = None

    def load(self) -> List[Dict[str, Any]]:
        """
        åŠ è½½æ•°æ®

        Returns:
            æ•°æ®åˆ—è¡¨
        """
        self._data = self._universal_dataset.load()
        return self._data

    def __len__(self) -> int:
        """è·å–æ•°æ®é›†å¤§å°"""
        if self._data is None:
            self.load()
        return len(self._data)


class LLMJudgeEvaluationTool:
    """LLM Judge è¯„ä¼°å·¥å…· - é€šç”¨ç‰ˆæœ¬

    æ”¯æŒå¤šç§æ•°æ®æºå’Œè¯„ä¼°æ¨¡æ¿çš„é«˜çº§è¯„ä¼°å·¥å…·ã€‚
    """

    def __init__(
        self,
        template: str = "math",
        llm: HelloAgentsLLM = None,
        field_mapping: Optional[Dict[str, str]] = None
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å·¥å…·

        Args:
            template: è¯„ä¼°æ¨¡æ¿ ("math" / "code" / "writing" / "qa")
            llm: LLMå®ä¾‹ï¼ˆå¿…éœ€ï¼Œåº”ç”±è°ƒç”¨è€…åˆ›å»ºï¼‰
            field_mapping: å­—æ®µæ˜ å°„
        """
        if llm is None:
            raise ValueError("llmå‚æ•°ä¸èƒ½ä¸ºç©ºï¼Œè¯·ä¼ å…¥HelloAgentsLLMå®ä¾‹")
        self.template = template
        self.llm = llm
        self.field_mapping = field_mapping or {"problem": "problem", "answer": "answer"}
        self.eval_config = EvaluationConfig.load_template(template)
        self.results = None

    def evaluate(self, data: List[Dict[str, Any]], max_samples: int = 0) -> Dict[str, Any]:
        """
        è¿è¡Œè¯„ä¼°

        Args:
            data: æ•°æ®åˆ—è¡¨
            max_samples: æœ€å¤šè¯„ä¼°æ ·æœ¬æ•° (0=å…¨éƒ¨)

        Returns:
            è¯„ä¼°ç»“æœ
        """
        # é™åˆ¶æ ·æœ¬æ•°
        data_to_evaluate = data[:max_samples] if max_samples > 0 else data

        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = BaseLLMJudgeEvaluator(
            llm=self.llm,
            eval_config=self.eval_config,
            field_mapping=self.field_mapping
        )

        # è¿è¡Œè¯„ä¼°
        self.results = evaluator.evaluate_batch(data_to_evaluate)
        return self.results

    def run(self, params: Dict[str, Any]) -> str:
        """
        Run complete evaluation workflow (high-level interface - local data only)

        Args:
            params: Parameter dictionary containing:
                - source_config: Data source config (required)
                  {"path": "path/to/data.json"}
                - field_mapping: Field mapping for data (optional)
                  {"problem": "question", "answer": "answer"}
                - reference_config: Reference data config (optional for comparison)
                  {"path": "path/to/reference.json"}
                - reference_field_mapping: Field mapping for reference data (optional)
                - template: Evaluation template (optional, overrides init template)
                - max_samples: Max samples to evaluate (optional, default: 0 = all)
                - output_dir: Output directory (optional, default: "evaluation_results")
                - judge_model: Judge model (optional, overrides init model)

        Returns:
            JSON string with evaluation results
        """
        import os
        from datetime import datetime

        # Parse parameters
        source_config = params.get("source_config")
        field_mapping = params.get("field_mapping", self.field_mapping)

        reference_config = params.get("reference_config")
        reference_field_mapping = params.get("reference_field_mapping", {})

        template = params.get("template", self.template)
        max_samples = params.get("max_samples", 0)
        output_dir = params.get("output_dir", "evaluation_results")

        # Validate required parameters
        if not source_config or "path" not in source_config:
            raise ValueError("source_config must contain 'path' key")

        # Add timestamp to output directory
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(output_dir, f"llm_judge_{timestamp}")

        # Create output directory
        os.makedirs(output_dir, exist_ok=True)

        print("\n" + "="*70)
        print("ğŸ¯ LLM Judge è¯„ä¼°")
        print("="*70)

        # Step 1: Load evaluation data
        print(f"\nğŸ“¥ æ­¥éª¤ 1: åŠ è½½è¯„ä¼°æ•°æ®")
        print(f"   Source: {source_config['path']}")
        dataset = UniversalDataset(
            source_config=source_config,
            field_mapping=field_mapping
        )
        data = dataset.load()

        # Step 2: Load reference data (optional)
        ref_data = None
        if reference_config and "path" in reference_config:
            print(f"\nğŸ“¥ æ­¥éª¤ 2: åŠ è½½å‚è€ƒæ•°æ®")
            print(f"   Source: {reference_config['path']}")
            reference_dataset = UniversalDataset(
                source_config=reference_config,
                field_mapping=reference_field_mapping
            )
            ref_data = reference_dataset.load()

        # Step 3: Configure evaluator
        print(f"\nâš™ï¸  æ­¥éª¤ 3: é…ç½®è¯„ä¼°å™¨")
        self.template = template
        self.field_mapping = field_mapping
        self.eval_config = EvaluationConfig.load_template(template)
        print(f"   Template: {template}")
        print(f"   Judge Model: {self.llm.model}")

        # Step 4: Run evaluation
        print(f"\nğŸš€ æ­¥éª¤ 4: å¼€å§‹è¯„ä¼°")
        results = self.evaluate(data, max_samples=max_samples)
        print(f"   âœ“ è¯„ä¼°å®Œæˆ")

        # Step 5: Export report
        print(f"\nğŸ’¾ æ­¥éª¤ 5: å¯¼å‡ºè¯„ä¼°æŠ¥å‘Š")
        report_path = self.export_report(results, output_dir=output_dir)
        print(f"   âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        print("\n" + "="*70)
        print("âœ… LLM Judge è¯„ä¼°å®Œæˆ")
        print("="*70)

        # Return results
        return json.dumps({
            "status": "success",
            "template": template,
            "judge_model": self.llm.model,
            "num_samples": len(data),
            "metrics": results.get("metrics", {}),
            "output_dir": output_dir
        }, ensure_ascii=False, indent=2)

    def export_report(self, results: Optional[Dict[str, Any]] = None, output_dir: str = "evaluation_results") -> str:
        """
        å¯¼å‡ºè¯„ä¼°æŠ¥å‘Š

        Args:
            results: è¯„ä¼°ç»“æœ (ä½¿ç”¨æœ€åä¸€æ¬¡evaluateçš„ç»“æœ)
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        if results is None:
            results = self.results

        if results is None:
            raise ValueError("æ²¡æœ‰è¯„ä¼°ç»“æœï¼Œè¯·å…ˆè¿è¡Œ evaluate() æ–¹æ³•")

        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        metrics = results.get("metrics", {})

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report_lines = [
            "# LLM Judge è¯„ä¼°æŠ¥å‘Š\n\n",
            f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
            f"**è¯„ä¼°æ¨¡æ¿**: {self.template}\n",
            f"**è¯„ä¼°æ ·æœ¬æ•°**: {len(results.get('results', []))}\n\n",
            "## æ€»ä½“è¯„åˆ†\n\n",
            f"- <strong>å¹³å‡æ€»åˆ†</strong>: {metrics.get('average_total_score', 0):.2f}/5.0\n",
            f"- <strong>é€šè¿‡ç‡</strong>: {metrics.get('pass_rate', 0):.1%} (â‰¥3.5åˆ†)\n",
            f"- <strong>ä¼˜ç§€ç‡</strong>: {metrics.get('excellent_rate', 0):.1%} (â‰¥4.5åˆ†)\n\n",
            "## å„ç»´åº¦è¯„åˆ†\n\n",
        ]

        # ç”Ÿæˆç»´åº¦è¯„åˆ†è¡¨æ ¼
        dimension_averages = metrics.get("dimension_averages", {})
        if dimension_averages:
            report_lines.append("| ç»´åº¦ | å¹³å‡åˆ† | è¯„çº§ |\n")
            report_lines.append("|------|--------|------|\n")

            for dim, score in dimension_averages.items():
                # æ ¹æ®åˆ†æ•°ç”Ÿæˆè¯„çº§
                if score >= 4.5:
                    rating = "ä¼˜ç§€ â­â­â­â­â­"
                elif score >= 4.0:
                    rating = "è‰¯å¥½ â­â­â­â­"
                elif score >= 3.5:
                    rating = "ä¸­ç­‰ â­â­â­"
                elif score >= 3.0:
                    rating = "åŠæ ¼ â­â­"
                else:
                    rating = "å¾…æ”¹è¿› â­"

                report_lines.append(f"| {dim} | {score:.2f}/5.0 | {rating} |\n")

            report_lines.append("\n")

        # ä¿å­˜MarkdownæŠ¥å‘Š
        report_path = output_path / "llm_judge_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.writelines(report_lines)

        # ä¿å­˜JSONç»“æœ
        results_file = output_path / "llm_judge_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        return str(report_path)

    def get_available_templates(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨æ¨¡æ¿"""
        return EvaluationConfig.get_available_templates()
