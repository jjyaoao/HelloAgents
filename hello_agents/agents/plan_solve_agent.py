"""Plan and Solve Agentå®ç° - åˆ†è§£è§„åˆ’ä¸é€æ­¥æ‰§è¡Œçš„æ™ºèƒ½ä½“"""

import json
from typing import Optional, List, Dict, TYPE_CHECKING, Any, AsyncGenerator

from ..core.agent import Agent
from ..core.llm import HelloAgentsLLM
from ..core.config import Config
from ..core.message import Message
from ..core.streaming import StreamEvent, StreamEventType
from ..core.lifecycle import LifecycleHook

if TYPE_CHECKING:
    from ..tools.registry import ToolRegistry

class Planner:
    """è§„åˆ’å™¨ - è´Ÿè´£å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºç®€å•æ­¥éª¤ï¼ˆä½¿ç”¨ Function Callingï¼‰"""

    def __init__(self, llm_client: HelloAgentsLLM, system_prompt: Optional[str] = None):
        self.llm_client = llm_client
        self.system_prompt = system_prompt or """ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„AIè§„åˆ’ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·æå‡ºçš„å¤æ‚é—®é¢˜åˆ†è§£æˆä¸€ä¸ªç”±å¤šä¸ªç®€å•æ­¥éª¤ç»„æˆçš„è¡ŒåŠ¨è®¡åˆ’ã€‚
è¯·ç¡®ä¿è®¡åˆ’ä¸­çš„æ¯ä¸ªæ­¥éª¤éƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ã€å¯æ‰§è¡Œçš„å­ä»»åŠ¡ï¼Œå¹¶ä¸”ä¸¥æ ¼æŒ‰ç…§é€»è¾‘é¡ºåºæ’åˆ—ã€‚"""

    def plan(self, question: str, **kwargs) -> List[str]:
        """
        ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ï¼ˆä½¿ç”¨ Function Callingï¼‰

        Args:
            question: è¦è§£å†³çš„é—®é¢˜
            **kwargs: LLMè°ƒç”¨å‚æ•°

        Returns:
            æ­¥éª¤åˆ—è¡¨
        """
        print("--- æ­£åœ¨ç”Ÿæˆè®¡åˆ’ ---")

        # å®šä¹‰è®¡åˆ’ç”Ÿæˆå·¥å…·
        plan_tool = {
            "type": "function",
            "function": {
                "name": "generate_plan",
                "description": "ç”Ÿæˆè§£å†³é—®é¢˜çš„åˆ†æ­¥è®¡åˆ’",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "steps": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "æŒ‰é¡ºåºæ’åˆ—çš„æ‰§è¡Œæ­¥éª¤åˆ—è¡¨"
                        }
                    },
                    "required": ["steps"]
                }
            }
        }

        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": f"è¯·ä¸ºä»¥ä¸‹é—®é¢˜ç”Ÿæˆè¯¦ç»†çš„æ‰§è¡Œè®¡åˆ’ï¼š\n\n{question}"}
        ]

        try:
            response = self.llm_client.invoke_with_tools(
                messages=messages,
                tools=[plan_tool],
                tool_choice={"type": "function", "function": {"name": "generate_plan"}},
                **kwargs
            )

            response_message = response.choices[0].message

            # æå–å·¥å…·è°ƒç”¨ç»“æœ
            if response_message.tool_calls:
                tool_call = response_message.tool_calls[0]
                arguments = json.loads(tool_call.function.arguments)
                plan = arguments.get("steps", [])

                print(f"âœ… è®¡åˆ’å·²ç”Ÿæˆ:")
                for i, step in enumerate(plan, 1):
                    print(f"  {i}. {step}")

                return plan
            else:
                print("âŒ æ¨¡å‹æœªè¿”å›è®¡åˆ’å·¥å…·è°ƒç”¨")
                return []

        except Exception as e:
            print(f"âŒ ç”Ÿæˆè®¡åˆ’æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []

class Executor:
    """æ‰§è¡Œå™¨ - è´Ÿè´£æŒ‰è®¡åˆ’é€æ­¥æ‰§è¡Œï¼ˆæ”¯æŒ Function Callingï¼‰"""

    def __init__(
        self,
        llm_client: HelloAgentsLLM,
        system_prompt: Optional[str] = None,
        tool_registry: Optional['ToolRegistry'] = None,
        enable_tool_calling: bool = True,
        max_tool_iterations: int = 3
    ):
        self.llm_client = llm_client
        self.system_prompt = system_prompt or """ä½ æ˜¯ä¸€ä½é¡¶çº§çš„AIæ‰§è¡Œä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸¥æ ¼æŒ‰ç…§ç»™å®šçš„è®¡åˆ’ï¼Œä¸€æ­¥æ­¥åœ°è§£å†³é—®é¢˜ã€‚
è¯·ä¸“æ³¨äºè§£å†³å½“å‰æ­¥éª¤ï¼Œå¹¶è¾“å‡ºè¯¥æ­¥éª¤çš„æœ€ç»ˆç­”æ¡ˆã€‚"""
        self.tool_registry = tool_registry
        self.enable_tool_calling = enable_tool_calling and tool_registry is not None
        self.max_tool_iterations = max_tool_iterations

    def execute(self, question: str, plan: List[str], **kwargs) -> str:
        """
        æŒ‰è®¡åˆ’æ‰§è¡Œä»»åŠ¡ï¼ˆæ”¯æŒ Function Callingï¼‰

        Args:
            question: åŸå§‹é—®é¢˜
            plan: æ‰§è¡Œè®¡åˆ’
            **kwargs: LLMè°ƒç”¨å‚æ•°

        Returns:
            æœ€ç»ˆç­”æ¡ˆ
        """
        history = []
        final_answer = ""

        print("\n--- æ­£åœ¨æ‰§è¡Œè®¡åˆ’ ---")
        for i, step in enumerate(plan, 1):
            print(f"\n-> æ­£åœ¨æ‰§è¡Œæ­¥éª¤ {i}/{len(plan)}: {step}")

            # æ„å»ºä¸Šä¸‹æ–‡æ¶ˆæ¯
            context = f"""# åŸå§‹é—®é¢˜:
{question}

# å®Œæ•´è®¡åˆ’:
{self._format_plan(plan)}

# å†å²æ­¥éª¤ä¸ç»“æœ:
{self._format_history(history) if history else "æ— "}

# å½“å‰æ­¥éª¤:
{step}

è¯·æ‰§è¡Œå½“å‰æ­¥éª¤å¹¶ç»™å‡ºç»“æœã€‚"""

            # æ‰§è¡Œå•ä¸ªæ­¥éª¤ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
            response_text = self._execute_step(context, **kwargs)

            history.append({"step": step, "result": response_text})
            final_answer = response_text
            print(f"âœ… æ­¥éª¤ {i} å·²å®Œæˆï¼Œç»“æœ: {final_answer}")

        return final_answer

    def _format_plan(self, plan: List[str]) -> str:
        """æ ¼å¼åŒ–è®¡åˆ’åˆ—è¡¨"""
        return "\n".join([f"{i}. {step}" for i, step in enumerate(plan, 1)])

    def _format_history(self, history: List[Dict[str, str]]) -> str:
        """æ ¼å¼åŒ–å†å²è®°å½•"""
        return "\n\n".join([f"æ­¥éª¤ {i}: {h['step']}\nç»“æœ: {h['result']}"
                           for i, h in enumerate(history, 1)])

    def _execute_step(self, context: str, **kwargs) -> str:
        """
        æ‰§è¡Œå•ä¸ªæ­¥éª¤ï¼ˆæ”¯æŒ Function Callingï¼‰

        Args:
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            æ­¥éª¤æ‰§è¡Œç»“æœ
        """
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": context}
        ]

        # å¦‚æœæ²¡æœ‰å¯ç”¨å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›
        if not self.enable_tool_calling or not self.tool_registry:
            llm_response = self.llm_client.invoke(messages, **kwargs)
            return llm_response.content if hasattr(llm_response, 'content') else str(llm_response)

        # å¯ç”¨å·¥å…·è°ƒç”¨æ¨¡å¼
        from .simple_agent import SimpleAgent
        # ä¸´æ—¶åˆ›å»ºä¸€ä¸ª SimpleAgent å®ä¾‹æ¥å¤ç”¨å·¥å…·è°ƒç”¨é€»è¾‘
        temp_agent = SimpleAgent(
            name="temp_executor",
            llm=self.llm_client,
            tool_registry=self.tool_registry
        )
        tool_schemas = temp_agent._build_tool_schemas()

        current_iteration = 0

        while current_iteration < self.max_tool_iterations:
            current_iteration += 1

            try:
                response = self.llm_client.invoke_with_tools(
                    messages=messages,
                    tools=tool_schemas,
                    tool_choice="auto",
                    **kwargs
                )
            except Exception as e:
                print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
                break

            response_message = response.choices[0].message

            # å¤„ç†å·¥å…·è°ƒç”¨
            tool_calls = response_message.tool_calls
            if not tool_calls:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›æ–‡æœ¬å“åº”
                return response_message.content or ""

            # å°†åŠ©æ‰‹æ¶ˆæ¯æ·»åŠ åˆ°å†å²
            messages.append({
                "role": "assistant",
                "content": response_message.content,
                "tool_calls": [
                    {
                        "id": tc.id,
                        "type": "function",
                        "function": {
                            "name": tc.function.name,
                            "arguments": tc.function.arguments
                        }
                    }
                    for tc in tool_calls
                ]
            })

            # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
            for tool_call in tool_calls:
                tool_name = tool_call.function.name
                tool_call_id = tool_call.id

                try:
                    arguments = json.loads(tool_call.function.arguments)
                except json.JSONDecodeError as e:
                    print(f"âŒ å·¥å…·å‚æ•°è§£æå¤±è´¥: {e}")
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call_id,
                        "content": f"é”™è¯¯ï¼šå‚æ•°æ ¼å¼ä¸æ­£ç¡® - {str(e)}"
                    })
                    continue

                # æ‰§è¡Œå·¥å…·ï¼ˆå¤ç”¨åŸºç±»æ–¹æ³•ï¼‰
                result = temp_agent._execute_tool_call(tool_name, arguments)

                # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call_id,
                    "content": result
                })

        # å¦‚æœè¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œè·å–æœ€åä¸€æ¬¡å›ç­”
        if current_iteration >= self.max_tool_iterations:
            llm_response = self.llm_client.invoke(messages, **kwargs)
            return llm_response.content if hasattr(llm_response, 'content') else str(llm_response)

        return ""

class PlanSolveAgent(Agent):
    """
    Plan and Solve Agent - åˆ†è§£è§„åˆ’ä¸é€æ­¥æ‰§è¡Œçš„æ™ºèƒ½ä½“

    è¿™ä¸ªAgentèƒ½å¤Ÿï¼š
    1. å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºç®€å•æ­¥éª¤ï¼ˆä½¿ç”¨ Function Callingï¼‰
    2. æŒ‰ç…§è®¡åˆ’é€æ­¥æ‰§è¡Œ
    3. ç»´æŠ¤æ‰§è¡Œå†å²å’Œä¸Šä¸‹æ–‡
    4. å¾—å‡ºæœ€ç»ˆç­”æ¡ˆ
    5. æ”¯æŒå·¥å…·è°ƒç”¨ï¼ˆå¯é€‰ï¼‰

    ç‰¹åˆ«é€‚åˆå¤šæ­¥éª¤æ¨ç†ã€æ•°å­¦é—®é¢˜ã€å¤æ‚åˆ†æç­‰ä»»åŠ¡ã€‚
    """

    def __init__(
        self,
        name: str,
        llm: HelloAgentsLLM,
        system_prompt: Optional[str] = None,
        config: Optional[Config] = None,
        planner_prompt: Optional[str] = None,
        executor_prompt: Optional[str] = None,
        tool_registry: Optional['ToolRegistry'] = None,
        enable_tool_calling: bool = True,
        max_tool_iterations: int = 3
    ):
        """
        åˆå§‹åŒ–PlanSolveAgent

        Args:
            name: Agentåç§°
            llm: LLMå®ä¾‹
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆAgentçº§åˆ«ï¼‰
            config: é…ç½®å¯¹è±¡
            planner_prompt: è§„åˆ’å™¨çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            executor_prompt: æ‰§è¡Œå™¨çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            tool_registry: å·¥å…·æ³¨å†Œè¡¨ï¼ˆå¯é€‰ï¼‰
            enable_tool_calling: æ˜¯å¦å¯ç”¨å·¥å…·è°ƒç”¨
            max_tool_iterations: æœ€å¤§å·¥å…·è°ƒç”¨è¿­ä»£æ¬¡æ•°
        """
        # ä¼ é€’ tool_registry åˆ°åŸºç±»
        super().__init__(
            name,
            llm,
            system_prompt,
            config,
            tool_registry=tool_registry
        )

        self.planner = Planner(self.llm, planner_prompt)
        self.executor = Executor(
            self.llm,
            executor_prompt,
            tool_registry=tool_registry,
            enable_tool_calling=enable_tool_calling,
            max_tool_iterations=max_tool_iterations
        )
    
    def run(self, input_text: str, **kwargs) -> str:
        """
        è¿è¡ŒPlan and Solve Agent
        
        Args:
            input_text: è¦è§£å†³çš„é—®é¢˜
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            æœ€ç»ˆç­”æ¡ˆ
        """
        print(f"\nğŸ¤– {self.name} å¼€å§‹å¤„ç†é—®é¢˜: {input_text}")
        
        # 1. ç”Ÿæˆè®¡åˆ’
        plan = self.planner.plan(input_text, **kwargs)
        if not plan:
            final_answer = "æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„è¡ŒåŠ¨è®¡åˆ’ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚"
            print(f"\n--- ä»»åŠ¡ç»ˆæ­¢ ---\n{final_answer}")
            
            # ä¿å­˜åˆ°å†å²è®°å½•
            self.add_message(Message(input_text, "user"))
            self.add_message(Message(final_answer, "assistant"))
            
            return final_answer
        
        # 2. æ‰§è¡Œè®¡åˆ’
        final_answer = self.executor.execute(input_text, plan, **kwargs)
        print(f"\n--- ä»»åŠ¡å®Œæˆ ---\næœ€ç»ˆç­”æ¡ˆ: {final_answer}")
        
        # ä¿å­˜åˆ°å†å²è®°å½•
        self.add_message(Message(input_text, "user"))
        self.add_message(Message(final_answer, "assistant"))

        return final_answer

    async def arun_stream(
        self,
        input_text: str,
        on_start: LifecycleHook = None,
        on_finish: LifecycleHook = None,
        on_error: LifecycleHook = None,
        **kwargs
    ) -> AsyncGenerator[StreamEvent, None]:
        """
        PlanAgent çœŸæ­£çš„æµå¼æ‰§è¡Œ

        å®æ—¶è¿”å›ï¼š
        - è§„åˆ’é˜¶æ®µçš„è®¡åˆ’ç”Ÿæˆ
        - æ‰§è¡Œé˜¶æ®µçš„æ¯ä¸ªæ­¥éª¤è¾“å‡º

        Args:
            input_text: ç”¨æˆ·è¾“å…¥
            on_start: å¼€å§‹é’©å­
            on_finish: å®Œæˆé’©å­
            on_error: é”™è¯¯é’©å­
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            StreamEvent: æµå¼äº‹ä»¶
        """
        # å‘é€å¼€å§‹äº‹ä»¶
        yield StreamEvent.create(
            StreamEventType.AGENT_START,
            self.name,
            input_text=input_text
        )

        try:
            # é˜¶æ®µ 1ï¼šè§„åˆ’
            yield StreamEvent.create(
                StreamEventType.STEP_START,
                self.name,
                phase="planning",
                description="ç”Ÿæˆæ‰§è¡Œè®¡åˆ’"
            )

            print(f"\nğŸ¤– {self.name} å¼€å§‹å¤„ç†é—®é¢˜: {input_text}")

            # ç”Ÿæˆè®¡åˆ’ï¼ˆåŒæ­¥æ–¹æ³•ï¼Œæš‚æ—¶ä¿æŒï¼‰
            plan = self.planner.plan(input_text, **kwargs)

            if not plan:
                error_msg = "æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„è¡ŒåŠ¨è®¡åˆ’ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚"

                yield StreamEvent.create(
                    StreamEventType.ERROR,
                    self.name,
                    error=error_msg,
                    phase="planning"
                )

                yield StreamEvent.create(
                    StreamEventType.AGENT_FINISH,
                    self.name,
                    result=error_msg
                )

                self.add_message(Message(input_text, "user"))
                self.add_message(Message(error_msg, "assistant"))
                return

            yield StreamEvent.create(
                StreamEventType.STEP_FINISH,
                self.name,
                phase="planning",
                plan=plan,
                total_steps=len(plan)
            )

            # é˜¶æ®µ 2ï¼šæ‰§è¡Œè®¡åˆ’
            step_results = []

            for i, step_description in enumerate(plan):
                step_num = i + 1

                # æ­¥éª¤å¼€å§‹
                yield StreamEvent.create(
                    StreamEventType.STEP_START,
                    self.name,
                    phase="execution",
                    step=step_num,
                    total_steps=len(plan),
                    description=step_description
                )

                print(f"\n--- æ­¥éª¤ {step_num}/{len(plan)} ---")
                print(f"ğŸ“‹ {step_description}")

                # æ„å»ºæ‰§è¡Œæç¤º
                context = "\n".join([
                    f"æ­¥éª¤ {j+1}: {plan[j]} -> {step_results[j]}"
                    for j in range(len(step_results))
                ])

                prompt = f"""åŸå§‹é—®é¢˜: {input_text}

å®Œæ•´è®¡åˆ’:
{chr(10).join([f"{j+1}. {s}" for j, s in enumerate(plan)])}

å·²å®Œæˆçš„æ­¥éª¤:
{context if context else "æ— "}

å½“å‰æ­¥éª¤: {step_description}

è¯·æ‰§è¡Œå½“å‰æ­¥éª¤å¹¶ç»™å‡ºç»“æœã€‚"""

                messages = [{"role": "user", "content": prompt}]

                # æµå¼æ‰§è¡Œæ­¥éª¤
                step_result = ""
                async for chunk in self.llm.astream_invoke(messages, **kwargs):
                    step_result += chunk

                    yield StreamEvent.create(
                        StreamEventType.LLM_CHUNK,
                        self.name,
                        chunk=chunk,
                        phase="execution",
                        step=step_num
                    )

                    print(chunk, end="", flush=True)

                print()  # æ¢è¡Œ

                step_results.append(step_result)

                # æ­¥éª¤å®Œæˆ
                yield StreamEvent.create(
                    StreamEventType.STEP_FINISH,
                    self.name,
                    phase="execution",
                    step=step_num,
                    result=step_result
                )

            # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            yield StreamEvent.create(
                StreamEventType.STEP_START,
                self.name,
                phase="final_answer",
                description="ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"
            )

            final_prompt = f"""åŸå§‹é—®é¢˜: {input_text}

æ‰§è¡Œè®¡åˆ’å’Œç»“æœ:
{chr(10).join([f"{i+1}. {plan[i]} -> {step_results[i]}" for i in range(len(plan))])}

è¯·åŸºäºä»¥ä¸Šæ­¥éª¤çš„æ‰§è¡Œç»“æœï¼Œç»™å‡ºåŸå§‹é—®é¢˜çš„æœ€ç»ˆç­”æ¡ˆã€‚"""

            final_messages = [{"role": "user", "content": final_prompt}]

            final_answer = ""
            async for chunk in self.llm.astream_invoke(final_messages, **kwargs):
                final_answer += chunk

                yield StreamEvent.create(
                    StreamEventType.LLM_CHUNK,
                    self.name,
                    chunk=chunk,
                    phase="final_answer"
                )

            # å‘é€å®Œæˆäº‹ä»¶
            yield StreamEvent.create(
                StreamEventType.AGENT_FINISH,
                self.name,
                result=final_answer,
                total_steps=len(plan)
            )

            print(f"\n--- ä»»åŠ¡å®Œæˆ ---\næœ€ç»ˆç­”æ¡ˆ: {final_answer}")

            # ä¿å­˜åˆ°å†å²
            self.add_message(Message(input_text, "user"))
            self.add_message(Message(final_answer, "assistant"))

        except Exception as e:
            # å‘é€é”™è¯¯äº‹ä»¶
            yield StreamEvent.create(
                StreamEventType.ERROR,
                self.name,
                error=str(e),
                error_type=type(e).__name__
            )
            raise
