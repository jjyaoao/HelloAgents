"""Reflection Agentå®ç° - è‡ªæˆ‘åæ€ä¸è¿­ä»£ä¼˜åŒ–çš„æ™ºèƒ½ä½“"""

from typing import Optional, List, Dict, Any, TYPE_CHECKING, AsyncGenerator
import json
from datetime import datetime

from ..core.agent import Agent
from ..core.llm import HelloAgentsLLM
from ..core.config import Config
from ..core.message import Message
from ..core.streaming import StreamEvent, StreamEventType
from ..core.lifecycle import LifecycleHook

if TYPE_CHECKING:
    from ..tools.registry import ToolRegistry

class Memory:
    """
    ç®€å•çš„çŸ­æœŸè®°å¿†æ¨¡å—ï¼Œç”¨äºå­˜å‚¨æ™ºèƒ½ä½“çš„è¡ŒåŠ¨ä¸åæ€è½¨è¿¹ã€‚
    """
    def __init__(self):
        self.records: List[Dict[str, Any]] = []

    def add_record(self, record_type: str, content: str):
        """å‘è®°å¿†ä¸­æ·»åŠ ä¸€æ¡æ–°è®°å½•"""
        self.records.append({"type": record_type, "content": content})
        print(f"ğŸ“ è®°å¿†å·²æ›´æ–°ï¼Œæ–°å¢ä¸€æ¡ '{record_type}' è®°å½•ã€‚")

    def get_trajectory(self) -> str:
        """å°†æ‰€æœ‰è®°å¿†è®°å½•æ ¼å¼åŒ–ä¸ºä¸€ä¸ªè¿è´¯çš„å­—ç¬¦ä¸²æ–‡æœ¬"""
        trajectory = ""
        for record in self.records:
            if record['type'] == 'execution':
                trajectory += f"--- ä¸Šä¸€è½®å°è¯• (ä»£ç ) ---\n{record['content']}\n\n"
            elif record['type'] == 'reflection':
                trajectory += f"--- è¯„å®¡å‘˜åé¦ˆ ---\n{record['content']}\n\n"
        return trajectory.strip()

    def get_last_execution(self) -> str:
        """è·å–æœ€è¿‘ä¸€æ¬¡çš„æ‰§è¡Œç»“æœ"""
        for record in reversed(self.records):
            if record['type'] == 'execution':
                return record['content']
        return ""

class ReflectionAgent(Agent):
    """
    Reflection Agent - è‡ªæˆ‘åæ€ä¸è¿­ä»£ä¼˜åŒ–çš„æ™ºèƒ½ä½“

    è¿™ä¸ªAgentèƒ½å¤Ÿï¼š
    1. æ‰§è¡Œåˆå§‹ä»»åŠ¡
    2. å¯¹ç»“æœè¿›è¡Œè‡ªæˆ‘åæ€
    3. æ ¹æ®åæ€ç»“æœè¿›è¡Œä¼˜åŒ–
    4. è¿­ä»£æ”¹è¿›ç›´åˆ°æ»¡æ„
    5. æ”¯æŒå·¥å…·è°ƒç”¨ï¼ˆå¯é€‰ï¼‰

    ç‰¹åˆ«é€‚åˆä»£ç ç”Ÿæˆã€æ–‡æ¡£å†™ä½œã€åˆ†ææŠ¥å‘Šç­‰éœ€è¦è¿­ä»£ä¼˜åŒ–çš„ä»»åŠ¡ã€‚

    ä½¿ç”¨æ ‡å‡† Function Calling æ ¼å¼ï¼Œé€šè¿‡ system_prompt å®šä¹‰è§’è‰²å’Œè¡Œä¸ºã€‚
    """

    def __init__(
        self,
        name: str,
        llm: HelloAgentsLLM,
        system_prompt: Optional[str] = None,
        config: Optional[Config] = None,
        max_iterations: int = 3,
        tool_registry: Optional['ToolRegistry'] = None,
        enable_tool_calling: bool = True,
        max_tool_iterations: int = 3
    ):
        """
        åˆå§‹åŒ–ReflectionAgent

        Args:
            name: Agentåç§°
            llm: LLMå®ä¾‹
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå®šä¹‰è§’è‰²å’Œåæ€ç­–ç•¥ï¼‰
            config: é…ç½®å¯¹è±¡
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            tool_registry: å·¥å…·æ³¨å†Œè¡¨ï¼ˆå¯é€‰ï¼‰
            enable_tool_calling: æ˜¯å¦å¯ç”¨å·¥å…·è°ƒç”¨
            max_tool_iterations: æœ€å¤§å·¥å…·è°ƒç”¨è¿­ä»£æ¬¡æ•°
        """
        # é»˜è®¤ system_prompt
        default_system_prompt = """ä½ æ˜¯ä¸€ä¸ªå…·æœ‰è‡ªæˆ‘åæ€èƒ½åŠ›çš„AIåŠ©æ‰‹ã€‚ä½ çš„å·¥ä½œæµç¨‹æ˜¯ï¼š
1. é¦–å…ˆå°è¯•å®Œæˆç”¨æˆ·çš„ä»»åŠ¡
2. ç„¶ååæ€ä½ çš„å›ç­”ï¼Œæ‰¾å‡ºå¯èƒ½çš„é—®é¢˜æˆ–æ”¹è¿›ç©ºé—´
3. æ ¹æ®åæ€ç»“æœä¼˜åŒ–ä½ çš„å›ç­”
4. å¦‚æœå›ç­”å·²ç»å¾ˆå¥½ï¼Œåœ¨åæ€æ—¶å›å¤"æ— éœ€æ”¹è¿›"

è¯·å§‹ç»ˆä¿æŒæ‰¹åˆ¤æ€§æ€ç»´ï¼Œè¿½æ±‚æ›´é«˜è´¨é‡çš„è¾“å‡ºã€‚"""

        # ä¼ é€’ tool_registry åˆ°åŸºç±»
        super().__init__(
            name,
            llm,
            system_prompt or default_system_prompt,
            config,
            tool_registry=tool_registry
        )
        self.max_iterations = max_iterations
        self.memory = Memory()
        self.enable_tool_calling = enable_tool_calling and tool_registry is not None
        self.max_tool_iterations = max_tool_iterations

    def run(self, input_text: str, **kwargs) -> str:
        """
        è¿è¡ŒReflection Agent

        Args:
            input_text: ä»»åŠ¡æè¿°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            æœ€ç»ˆä¼˜åŒ–åçš„ç»“æœ
        """
        print(f"\nğŸ¤– {self.name} å¼€å§‹å¤„ç†ä»»åŠ¡: {input_text}")

        # é‡ç½®è®°å¿†
        self.memory = Memory()

        # 1. åˆå§‹æ‰§è¡Œ
        print("\n--- æ­£åœ¨è¿›è¡Œåˆå§‹å°è¯• ---")
        initial_result = self._execute_task(input_text, **kwargs)
        self.memory.add_record("execution", initial_result)

        # 2. è¿­ä»£å¾ªç¯ï¼šåæ€ä¸ä¼˜åŒ–
        for i in range(self.max_iterations):
            print(f"\n--- ç¬¬ {i+1}/{self.max_iterations} è½®è¿­ä»£ ---")

            # a. åæ€
            print("\n-> æ­£åœ¨è¿›è¡Œåæ€...")
            last_result = self.memory.get_last_execution()
            feedback = self._reflect_on_result(input_text, last_result, **kwargs)
            self.memory.add_record("reflection", feedback)

            # b. æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
            if "æ— éœ€æ”¹è¿›" in feedback or "no need for improvement" in feedback.lower():
                print("\nâœ… åæ€è®¤ä¸ºç»“æœå·²æ— éœ€æ”¹è¿›ï¼Œä»»åŠ¡å®Œæˆã€‚")
                break

            # c. ä¼˜åŒ–
            print("\n-> æ­£åœ¨è¿›è¡Œä¼˜åŒ–...")
            refined_result = self._refine_result(input_text, last_result, feedback, **kwargs)
            self.memory.add_record("execution", refined_result)

        final_result = self.memory.get_last_execution()
        print(f"\n--- ä»»åŠ¡å®Œæˆ ---\næœ€ç»ˆç»“æœ:\n{final_result}")

        # ä¿å­˜åˆ°å†å²è®°å½•
        self.add_message(Message(input_text, "user"))
        self.add_message(Message(final_result, "assistant"))

        return final_result

    def _execute_task(self, task: str, **kwargs) -> str:
        """æ‰§è¡Œåˆå§‹ä»»åŠ¡"""
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": f"è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š\n\n{task}"}
        ]
        return self._get_llm_response(messages, **kwargs)

    def _reflect_on_result(self, task: str, result: str, **kwargs) -> str:
        """å¯¹ç»“æœè¿›è¡Œåæ€"""
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": f"""è¯·ä»”ç»†å®¡æŸ¥ä»¥ä¸‹å›ç­”ï¼Œå¹¶æ‰¾å‡ºå¯èƒ½çš„é—®é¢˜æˆ–æ”¹è¿›ç©ºé—´ï¼š

# åŸå§‹ä»»åŠ¡:
{task}

# å½“å‰å›ç­”:
{result}

è¯·åˆ†æè¿™ä¸ªå›ç­”çš„è´¨é‡ï¼ŒæŒ‡å‡ºä¸è¶³ä¹‹å¤„ï¼Œå¹¶æå‡ºå…·ä½“çš„æ”¹è¿›å»ºè®®ã€‚
å¦‚æœå›ç­”å·²ç»å¾ˆå¥½ï¼Œè¯·å›ç­”"æ— éœ€æ”¹è¿›"ã€‚"""}
        ]
        return self._get_llm_response(messages, **kwargs)

    def _refine_result(self, task: str, last_attempt: str, feedback: str, **kwargs) -> str:
        """æ ¹æ®åé¦ˆä¼˜åŒ–ç»“æœ"""
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": f"""è¯·æ ¹æ®åé¦ˆæ„è§æ”¹è¿›ä½ çš„å›ç­”ï¼š

# åŸå§‹ä»»åŠ¡:
{task}

# ä¸Šä¸€è½®å›ç­”:
{last_attempt}

# åé¦ˆæ„è§:
{feedback}

è¯·æä¾›ä¸€ä¸ªæ”¹è¿›åçš„å›ç­”ã€‚"""}
        ]
        return self._get_llm_response(messages, **kwargs)

    def _get_llm_response(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """
        è°ƒç”¨LLMå¹¶è·å–å®Œæ•´å“åº”ï¼ˆæ”¯æŒ Function Callingï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            LLMå“åº”æ–‡æœ¬
        """
        # å¦‚æœæ²¡æœ‰å¯ç”¨å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›
        if not self.enable_tool_calling or not self.tool_registry:
            llm_response = self.llm.invoke(messages, **kwargs)
            return llm_response.content if hasattr(llm_response, 'content') else str(llm_response)

        # å¯ç”¨å·¥å…·è°ƒç”¨æ¨¡å¼
        tool_schemas = self._build_tool_schemas()
        current_iteration = 0

        while current_iteration < self.max_tool_iterations:
            current_iteration += 1

            try:
                response = self.llm.invoke_with_tools(
                    messages=messages,
                    tools=tool_schemas,
                    tool_choice="auto",
                    **kwargs
                )
            except Exception as e:
                print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
                break

            response_message = response.choices[0].message

            # å¤„ç†å·¥å…·è°ƒç”¨
            tool_calls = response_message.tool_calls
            if not tool_calls:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›æ–‡æœ¬å“åº”
                return response_message.content or ""

            # å°†åŠ©æ‰‹æ¶ˆæ¯æ·»åŠ åˆ°å†å²
            messages.append({
                "role": "assistant",
                "content": response_message.content,
                "tool_calls": [
                    {
                        "id": tc.id,
                        "type": "function",
                        "function": {
                            "name": tc.function.name,
                            "arguments": tc.function.arguments
                        }
                    }
                    for tc in tool_calls
                ]
            })

            # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
            for tool_call in tool_calls:
                tool_name = tool_call.function.name
                tool_call_id = tool_call.id

                try:
                    arguments = json.loads(tool_call.function.arguments)
                except json.JSONDecodeError as e:
                    print(f"âŒ å·¥å…·å‚æ•°è§£æå¤±è´¥: {e}")
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call_id,
                        "content": f"é”™è¯¯ï¼šå‚æ•°æ ¼å¼ä¸æ­£ç¡® - {str(e)}"
                    })
                    continue

                # æ‰§è¡Œå·¥å…·ï¼ˆå¤ç”¨åŸºç±»æ–¹æ³•ï¼‰
                result = self._execute_tool_call(tool_name, arguments)

                # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call_id,
                    "content": result
                })

        # å¦‚æœè¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œè·å–æœ€åä¸€æ¬¡å›ç­”
        if current_iteration >= self.max_tool_iterations:
            llm_response = self.llm.invoke(messages, **kwargs)
            return llm_response.content if hasattr(llm_response, 'content') else str(llm_response)

        return ""

    async def arun_stream(
        self,
        input_text: str,
        on_start: LifecycleHook = None,
        on_finish: LifecycleHook = None,
        on_error: LifecycleHook = None,
        **kwargs
    ) -> AsyncGenerator[StreamEvent, None]:
        """
        ReflectionAgent çœŸæ­£çš„æµå¼æ‰§è¡Œ

        å®æ—¶è¿”å›ï¼š
        - åˆå§‹æ‰§è¡Œé˜¶æ®µçš„ LLM è¾“å‡º
        - åæ€é˜¶æ®µçš„æ€è€ƒè¿‡ç¨‹
        - ä¼˜åŒ–é˜¶æ®µçš„ LLM è¾“å‡º

        Args:
            input_text: ç”¨æˆ·è¾“å…¥
            on_start: å¼€å§‹é’©å­
            on_finish: å®Œæˆé’©å­
            on_error: é”™è¯¯é’©å­
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            StreamEvent: æµå¼äº‹ä»¶
        """
        # å‘é€å¼€å§‹äº‹ä»¶
        yield StreamEvent.create(
            StreamEventType.AGENT_START,
            self.name,
            input_text=input_text
        )

        try:
            # é˜¶æ®µ 1ï¼šåˆå§‹æ‰§è¡Œ
            yield StreamEvent.create(
                StreamEventType.STEP_START,
                self.name,
                phase="initial_execution",
                description="ç”Ÿæˆåˆå§‹å›ç­”"
            )

            messages = []
            if self.system_prompt:
                messages.append({"role": "system", "content": self.system_prompt})

            for msg in self._history:
                messages.append({"role": msg.role, "content": msg.content})

            messages.append({"role": "user", "content": input_text})

            # æµå¼è·å–åˆå§‹å›ç­”
            initial_response = ""
            async for chunk in self.llm.astream_invoke(messages, **kwargs):
                initial_response += chunk
                yield StreamEvent.create(
                    StreamEventType.LLM_CHUNK,
                    self.name,
                    chunk=chunk,
                    phase="execution"
                )

            yield StreamEvent.create(
                StreamEventType.STEP_FINISH,
                self.name,
                phase="initial_execution",
                result=initial_response
            )

            # é˜¶æ®µ 2ï¼šåæ€ä¸ä¼˜åŒ–å¾ªç¯
            current_response = initial_response

            for iteration in range(self.max_iterations):
                # åæ€é˜¶æ®µ
                yield StreamEvent.create(
                    StreamEventType.STEP_START,
                    self.name,
                    phase="reflection",
                    iteration=iteration + 1,
                    description=f"ç¬¬ {iteration + 1} æ¬¡åæ€"
                )

                reflection_prompt = self._build_reflection_prompt(input_text, current_response)
                reflection_messages = [{"role": "user", "content": reflection_prompt}]

                reflection = ""
                async for chunk in self.llm.astream_invoke(reflection_messages, **kwargs):
                    reflection += chunk
                    yield StreamEvent.create(
                        StreamEventType.THINKING,
                        self.name,
                        chunk=chunk,
                        phase="reflection",
                        iteration=iteration + 1
                    )

                yield StreamEvent.create(
                    StreamEventType.STEP_FINISH,
                    self.name,
                    phase="reflection",
                    iteration=iteration + 1,
                    reflection=reflection
                )

                # ä¼˜åŒ–é˜¶æ®µ
                yield StreamEvent.create(
                    StreamEventType.STEP_START,
                    self.name,
                    phase="refinement",
                    iteration=iteration + 1,
                    description=f"ç¬¬ {iteration + 1} æ¬¡ä¼˜åŒ–"
                )

                refinement_prompt = self._build_refinement_prompt(
                    input_text,
                    current_response,
                    reflection
                )
                refinement_messages = [{"role": "user", "content": refinement_prompt}]

                refined_response = ""
                async for chunk in self.llm.astream_invoke(refinement_messages, **kwargs):
                    refined_response += chunk
                    yield StreamEvent.create(
                        StreamEventType.LLM_CHUNK,
                        self.name,
                        chunk=chunk,
                        phase="refinement",
                        iteration=iteration + 1
                    )

                yield StreamEvent.create(
                    StreamEventType.STEP_FINISH,
                    self.name,
                    phase="refinement",
                    iteration=iteration + 1,
                    result=refined_response
                )

                current_response = refined_response

            # å‘é€å®Œæˆäº‹ä»¶
            yield StreamEvent.create(
                StreamEventType.AGENT_FINISH,
                self.name,
                result=current_response,
                total_iterations=self.max_iterations
            )

            # ä¿å­˜åˆ°å†å²
            self.add_message(Message(input_text, "user"))
            self.add_message(Message(current_response, "assistant"))

        except Exception as e:
            # å‘é€é”™è¯¯äº‹ä»¶
            yield StreamEvent.create(
                StreamEventType.ERROR,
                self.name,
                error=str(e),
                error_type=type(e).__name__
            )
            raise
