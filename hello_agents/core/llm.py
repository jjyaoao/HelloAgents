"""HelloAgentsç»Ÿä¸€LLMæ¥å£ - æ”¯æŒOpenAIã€Anthropicã€Geminiç­‰å¤šç§æ¥å£"""

import os
import asyncio
from typing import Optional, Iterator, List, Dict, Union, Any, AsyncIterator

from .exceptions import HelloAgentsException
from .llm_response import LLMResponse, StreamStats
from .llm_adapters import create_adapter, BaseLLMAdapter


class HelloAgentsLLM:
    """
    HelloAgentsç»Ÿä¸€LLMå®¢æˆ·ç«¯

    è®¾è®¡ç†å¿µï¼š
    - ç»Ÿä¸€é…ç½®ï¼šåªéœ€ LLM_MODEL_IDã€LLM_API_KEYã€LLM_BASE_URLã€LLM_TIMEOUT
    - è‡ªåŠ¨é€‚é…ï¼šæ ¹æ®base_urlè‡ªåŠ¨é€‰æ‹©é€‚é…å™¨ï¼ˆOpenAI/Anthropic/Geminiï¼‰
    - ç»Ÿè®¡ä¿¡æ¯ï¼šè¿”å›tokenä½¿ç”¨é‡ã€è€—æ—¶ç­‰ä¿¡æ¯ï¼Œæ–¹ä¾¿æ—¥å¿—è®°å½•
    - Thinking Modelï¼šè‡ªåŠ¨è¯†åˆ«å¹¶å¤„ç†æ¨ç†è¿‡ç¨‹ï¼ˆo1ã€deepseek-reasonerç­‰ï¼‰

    æ”¯æŒçš„æ¥å£ï¼š
    - OpenAIåŠæ‰€æœ‰å…¼å®¹æ¥å£ï¼ˆDeepSeekã€Qwenã€Kimiã€æ™ºè°±ã€Ollamaç­‰ï¼‰
    - Anthropic Claude
    - Google Gemini
    """

    def __init__(
        self,
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        timeout: Optional[int] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯

        å‚æ•°ä¼˜å…ˆçº§ï¼šä¼ å…¥å‚æ•° > ç¯å¢ƒå˜é‡

        Args:
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä» LLM_MODEL_ID è¯»å–
            api_key: APIå¯†é’¥ï¼Œé»˜è®¤ä» LLM_API_KEY è¯»å–
            base_url: æœåŠ¡åœ°å€ï¼Œé»˜è®¤ä» LLM_BASE_URL è¯»å–
            temperature: æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤0.7
            max_tokens: æœ€å¤§tokenæ•°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä» LLM_TIMEOUT è¯»å–ï¼Œé»˜è®¤60ç§’
        """
        # åŠ è½½é…ç½®
        self.model = model or os.getenv("LLM_MODEL_ID")
        self.api_key = api_key or os.getenv("LLM_API_KEY")
        self.base_url = base_url or os.getenv("LLM_BASE_URL")
        self.timeout = timeout or int(os.getenv("LLM_TIMEOUT", "60"))

        self.temperature = temperature
        self.max_tokens = max_tokens
        self.kwargs = kwargs

        # éªŒè¯å¿…è¦å‚æ•°
        if not self.model:
            raise HelloAgentsException("å¿…é¡»æä¾›æ¨¡å‹åç§°ï¼ˆmodelå‚æ•°æˆ–LLM_MODEL_IDç¯å¢ƒå˜é‡ï¼‰")
        if not self.api_key:
            raise HelloAgentsException("å¿…é¡»æä¾›APIå¯†é’¥ï¼ˆapi_keyå‚æ•°æˆ–LLM_API_KEYç¯å¢ƒå˜é‡ï¼‰")
        if not self.base_url:
            raise HelloAgentsException("å¿…é¡»æä¾›æœåŠ¡åœ°å€ï¼ˆbase_urlå‚æ•°æˆ–LLM_BASE_URLç¯å¢ƒå˜é‡ï¼‰")

        # åˆ›å»ºé€‚é…å™¨ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
        self._adapter: BaseLLMAdapter = create_adapter(
            api_key=self.api_key,
            base_url=self.base_url,
            timeout=self.timeout,
            model=self.model
        )

        # æœ€åä¸€æ¬¡è°ƒç”¨çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºæµå¼è°ƒç”¨ï¼‰
        self.last_call_stats: Optional[StreamStats] = None

    def think(self, messages: List[Dict[str, str]], temperature: Optional[float] = None) -> Iterator[str]:
        """
        è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ€è€ƒï¼Œå¹¶è¿”å›æµå¼å“åº”ã€‚
        è¿™æ˜¯ä¸»è¦çš„è°ƒç”¨æ–¹æ³•ï¼Œé»˜è®¤ä½¿ç”¨æµå¼å“åº”ä»¥è·å¾—æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„å€¼

        Yields:
            str: æµå¼å“åº”çš„æ–‡æœ¬ç‰‡æ®µ

        Note:
            æµå¼è°ƒç”¨ç»“æŸåï¼Œå¯é€šè¿‡ llm.last_call_stats è·å–ç»Ÿè®¡ä¿¡æ¯
        """
        print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...")

        # å‡†å¤‡å‚æ•°
        kwargs = {
            "temperature": temperature if temperature is not None else self.temperature,
        }
        if self.max_tokens:
            kwargs["max_tokens"] = self.max_tokens

        try:
            print("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:")
            for chunk in self._adapter.stream_invoke(messages, **kwargs):
                print(chunk, end="", flush=True)
                yield chunk
            print()  # æ¢è¡Œ

            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            if hasattr(self._adapter, 'last_stats'):
                self.last_call_stats = self._adapter.last_stats

        except Exception as e:
            print(f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise

    def invoke(self, messages: List[Dict[str, str]], **kwargs) -> LLMResponse:
        """
        éæµå¼è°ƒç”¨LLMï¼Œè¿”å›å®Œæ•´å“åº”å¯¹è±¡ã€‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆtemperature, max_tokensç­‰ï¼‰

        Returns:
            LLMResponse: åŒ…å«å†…å®¹ã€ç»Ÿè®¡ä¿¡æ¯ã€æ¨ç†è¿‡ç¨‹ï¼ˆthinking modelï¼‰çš„å“åº”å¯¹è±¡

        Example:
            response = llm.invoke([{"role": "user", "content": "ä½ å¥½"}])
            print(response.content)  # å›å¤å†…å®¹
            print(response.usage)    # tokenä½¿ç”¨é‡
            print(response.latency_ms)  # è€—æ—¶
            if response.reasoning_content:  # thinking modelçš„æ¨ç†è¿‡ç¨‹
                print(response.reasoning_content)
        """
        # åˆå¹¶å‚æ•°
        call_kwargs = {
            "temperature": kwargs.pop("temperature", self.temperature),
        }
        if self.max_tokens:
            call_kwargs["max_tokens"] = kwargs.pop("max_tokens", self.max_tokens)
        call_kwargs.update(kwargs)

        return self._adapter.invoke(messages, **call_kwargs)

    def stream_invoke(self, messages: List[Dict[str, str]], **kwargs) -> Iterator[str]:
        """
        æµå¼è°ƒç”¨LLMçš„åˆ«åæ–¹æ³•ï¼Œä¸thinkæ–¹æ³•åŠŸèƒ½ç›¸åŒã€‚
        ä¿æŒå‘åå…¼å®¹æ€§ã€‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            str: æµå¼å“åº”çš„æ–‡æœ¬ç‰‡æ®µ

        Note:
            æµå¼è°ƒç”¨ç»“æŸåï¼Œå¯é€šè¿‡ llm.last_call_stats è·å–ç»Ÿè®¡ä¿¡æ¯
        """
        temperature = kwargs.pop("temperature", None)

        # å‡†å¤‡å‚æ•°
        call_kwargs = {}
        if self.max_tokens:
            call_kwargs["max_tokens"] = kwargs.pop("max_tokens", self.max_tokens)
        call_kwargs.update(kwargs)

        for chunk in self._adapter.stream_invoke(messages, temperature=temperature, **call_kwargs):
            yield chunk

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        if hasattr(self._adapter, 'last_stats'):
            self.last_call_stats = self._adapter.last_stats

    def invoke_with_tools(
        self,
        messages: List[Dict],
        tools: List[Dict],
        tool_choice: Union[str, Dict] = "auto",
        **kwargs
    ) -> Any:
        """
        è°ƒç”¨ LLM å¹¶æ”¯æŒå·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰

        è¿™æ˜¯æ”¯æŒ OpenAI Function Calling çš„æ ¸å¿ƒæ–¹æ³•ï¼Œç”¨äºç»“æ„åŒ–å·¥å…·è°ƒç”¨ã€‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"role": "user", "content": "..."}]
            tools: å·¥å…· schema åˆ—è¡¨ï¼Œæ ¼å¼ä¸º OpenAI Function Calling è§„èŒƒ
            tool_choice: å·¥å…·é€‰æ‹©ç­–ç•¥
                - "auto": è®©æ¨¡å‹è‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·ï¼ˆé»˜è®¤ï¼‰
                - "none": å¼ºåˆ¶ä¸è°ƒç”¨å·¥å…·
                - "required": å¼ºåˆ¶è°ƒç”¨å·¥å…·
                - {"type": "function", "function": {"name": "tool_name"}}: å¼ºåˆ¶è°ƒç”¨æŒ‡å®šå·¥å…·
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆtemperature, max_tokens ç­‰ï¼‰

        Returns:
            åŸç”Ÿå“åº”å¯¹è±¡ï¼ŒåŒ…å« tool_calls ä¿¡æ¯

        Raises:
            HelloAgentsException: å½“ LLM è°ƒç”¨å¤±è´¥æ—¶
        """
        # åˆå¹¶å‚æ•°
        call_kwargs = {
            "temperature": kwargs.pop("temperature", self.temperature),
            "tool_choice": tool_choice,
        }
        if self.max_tokens:
            call_kwargs["max_tokens"] = kwargs.pop("max_tokens", self.max_tokens)
        call_kwargs.update(kwargs)

        return self._adapter.invoke_with_tools(messages, tools, **call_kwargs)

    # ==================== å¼‚æ­¥æ–¹æ³• ====================

    async def ainvoke(self, messages: List[Dict[str, str]], **kwargs) -> LLMResponse:
        """
        å¼‚æ­¥éæµå¼è°ƒç”¨ LLM

        åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥ invoke æ–¹æ³•ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆtemperature, max_tokensç­‰ï¼‰

        Returns:
            LLMResponse: åŒ…å«å†…å®¹ã€ç»Ÿè®¡ä¿¡æ¯çš„å“åº”å¯¹è±¡

        Example:
            response = await llm.ainvoke([{"role": "user", "content": "ä½ å¥½"}])
            print(response.content)
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            lambda: self.invoke(messages, **kwargs)
        )

    async def astream_invoke(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> AsyncIterator[str]:
        """
        çœŸæ­£çš„å¼‚æ­¥æµå¼è°ƒç”¨ LLMï¼ˆä½¿ç”¨ adapter çš„å¼‚æ­¥å®ç°ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            str: æµå¼å“åº”çš„æ–‡æœ¬ç‰‡æ®µï¼ˆå®æ—¶è¿”å›ï¼‰

        Example:
            async for chunk in llm.astream_invoke(messages):
                print(chunk, end="", flush=True)
        """
        # ä½¿ç”¨ adapter çš„å¼‚æ­¥æµå¼æ–¹æ³•
        async for chunk in self._adapter.astream_invoke(messages, **kwargs):
            yield chunk

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        if hasattr(self._adapter, 'last_stats'):
            self.last_call_stats = self._adapter.last_stats

    async def ainvoke_with_tools(
        self,
        messages: List[Dict],
        tools: List[Dict],
        tool_choice: Union[str, Dict] = "auto",
        **kwargs
    ) -> Any:
        """
        å¼‚æ­¥è°ƒç”¨ LLM å¹¶æ”¯æŒå·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            tools: å·¥å…· schema åˆ—è¡¨
            tool_choice: å·¥å…·é€‰æ‹©ç­–ç•¥
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            åŸç”Ÿå“åº”å¯¹è±¡ï¼ŒåŒ…å« tool_calls ä¿¡æ¯
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            lambda: self.invoke_with_tools(messages, tools, tool_choice, **kwargs)
        )
