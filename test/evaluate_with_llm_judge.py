"""
LLM Judge é€šç”¨è¯„ä¼°å™¨ - çœŸå®åœºæ™¯ä¸¤å±‚çº§å¯¹æ¯”æµ‹è¯•

æœ¬è„šæœ¬å±•ç¤ºé€šç”¨ LLM Judge è¯„ä¼°å™¨åœ¨å®é™…åº”ç”¨ä¸­çš„å„ç§ä½¿ç”¨æ–¹å¼ã€‚

å…³é”®ç‰¹ç‚¹ï¼š
1. å¤šæ¨¡æ¿æ”¯æŒï¼šmath, code, writing, qa
2. å­—æ®µæ˜ å°„ï¼šå¤„ç†ä¸åŒæ•°æ®æ ¼å¼çš„è‡ªé€‚åº”å¯¹é½
3. è‡ªå®šä¹‰ç»´åº¦ï¼šæ ¹æ®æ¨¡æ¿è‡ªåŠ¨è°ƒæ•´è¯„ä¼°ç»´åº¦
4. çœŸå®åœºæ™¯ï¼šæ¨¡æ‹Ÿå®é™…ä½¿ç”¨ä¸­çš„å„ç§æƒ…å†µ

æ¡ˆä¾‹ 1ï¼šç›´æ¥ä½¿ç”¨è¯„ä¼°å™¨ï¼ˆä¸‰ä¸ªçœŸå®åœºæ™¯ï¼‰
  åœºæ™¯ Aï¼šæ•°å­¦é¢˜è¯„ä¼°ï¼ˆæ ‡å‡†å­—æ®µ + math æ¨¡æ¿ï¼‰
  åœºæ™¯ Bï¼šä»£ç è¯„ä¼°ï¼ˆå­—æ®µæ˜ å°„ + code æ¨¡æ¿ï¼‰
  åœºæ™¯ Cï¼šå†™ä½œè¯„ä¼°ï¼ˆå­—æ®µæ˜ å°„ + writing æ¨¡æ¿ï¼‰

æ¡ˆä¾‹ 2ï¼šä½¿ç”¨é«˜çº§å·¥å…·ï¼ˆä¸€è¡Œæå®šï¼‰
  è‡ªåŠ¨å¤„ç†æ•°æ®åŠ è½½ã€å­—æ®µæ˜ å°„ã€æ¨¡æ¿åˆ‡æ¢ã€æŠ¥å‘Šç”Ÿæˆ
"""

import sys
import os
import json
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from hello_agents import (
    HelloAgentsLLM,
    UniversalLLMJudgeEvaluator,
    UniversalLLMJudgeTool,
)
from hello_agents.evaluation.benchmarks.data_generation_Universal.evaluation_config import (
    EvaluationConfig,
)

# ============================================================================
# æµ‹è¯•æ•°æ®å‡†å¤‡ï¼ˆçœŸå®åœºæ™¯æ•°æ®ï¼ŒåŒ…å«ä¸åŒå­—æ®µåï¼‰
# ============================================================================


def prepare_test_data():
    """å‡†å¤‡ä¸‰ç§ä¸åŒæ¨¡æ¿çš„æµ‹è¯•æ•°æ®"""

    # åœºæ™¯ Aï¼šæ•°å­¦é¢˜ï¼ˆæ ‡å‡†å­—æ®µåï¼‰
    math_problems = [
        {
            "id": "math_001",
            "problem": "Find the number of positive integers $n$ such that $n^2 + 19n + 92$ is a perfect square.",
            "answer": "4",
            "solution": "Let $n^2 + 19n + 92 = m^2$. Completing the square: $(2n+19)^2 - 4m^2 = -7$. This gives us the Pell-like equation with limited solutions."
        },
        {
            "id": "math_002",
            "problem": "In triangle ABC with sides AB=13, BC=14, CA=15, find the area.",
            "answer": "84",
            "solution": "Using Heron's formula: $s=21$, Area $= \\sqrt{21 \\cdot 8 \\cdot 7 \\cdot 6} = 84$."
        }
    ]

    # åœºæ™¯ Bï¼šä»£ç è¯„ä¼°ï¼ˆéæ ‡å‡†å­—æ®µï¼šcode, expected_outputï¼‰
    code_problems = [
        {
            "id": "code_001",
            "code": """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
            """,
            "expected_output": "Returns correct fibonacci numbers",
            "context": "Write an optimized solution"
        },
        {
            "id": "code_002",
            "code": """
def sort_array(arr):
    for i in range(len(arr)):
        for j in range(len(arr)-1-i):
            if arr[j] > arr[j+1]:
                arr[j], arr[j+1] = arr[j+1], arr[j]
    return arr
            """,
            "expected_output": "Correctly sorted array in ascending order",
            "context": "Bubble sort implementation"
        }
    ]

    # åœºæ™¯ Cï¼šå†™ä½œè¯„ä¼°ï¼ˆéæ ‡å‡†å­—æ®µï¼šcontent, targetï¼‰
    writing_problems = [
        {
            "id": "writing_001",
            "content": "Python is a high-level, interpreted programming language known for its simplicity and readability. It supports multiple programming paradigms including procedural, object-oriented, and functional programming.",
            "target": "Technical article about Python",
            "length": "short"
        },
        {
            "id": "writing_002",
            "content": "Machine learning is revolutionizing how we approach data analysis. With algorithms that can learn patterns from data, ML enables computers to make predictions without explicit programming.",
            "target": "Educational blog post about ML",
            "length": "medium"
        }
    ]

    return math_problems, code_problems, writing_problems


def save_test_data():
    """ä¿å­˜æµ‹è¯•æ•°æ®åˆ°æ–‡ä»¶"""
    math_data, code_data, writing_data = prepare_test_data()

    os.makedirs("./test_data", exist_ok=True)

    with open("./test_data/math_problems.json", 'w', encoding='utf-8') as f:
        json.dump(math_data, f, indent=2, ensure_ascii=False)

    with open("./test_data/code_problems.json", 'w', encoding='utf-8') as f:
        json.dump(code_data, f, indent=2, ensure_ascii=False)

    with open("./test_data/writing_problems.json", 'w', encoding='utf-8') as f:
        json.dump(writing_data, f, indent=2, ensure_ascii=False)

    return math_data, code_data, writing_data


# ============================================================================
# æ¡ˆä¾‹ 1ï¼šç›´æ¥ä½¿ç”¨è¯„ä¼°å™¨ï¼ˆä¸‰ä¸ªçœŸå®åœºæ™¯ï¼‰
# ============================================================================


def example_scene_a_math():
    """
    åœºæ™¯ Aï¼šæ•°å­¦é¢˜è¯„ä¼°

    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨ "math" æ¨¡æ¿
    - æ ‡å‡†å­—æ®µåï¼ˆproblem, answer, solutionï¼‰
    - æ— éœ€å­—æ®µæ˜ å°„
    - ç»´åº¦ï¼šcorrectness, clarity, completeness, difficulty_match, originality
    """
    print("\n" + "="*70)
    print("ğŸ“Œ æ¡ˆä¾‹ 1 - åœºæ™¯ Aï¼šæ•°å­¦é¢˜è¯„ä¼°ï¼ˆæ ‡å‡†å­—æ®µ + math æ¨¡æ¿ï¼‰")
    print("="*70)

    from hello_agents import HelloAgentsLLM, UniversalLLMJudgeEvaluator
    from hello_agents.evaluation.benchmarks.data_generation_Universal.evaluation_config import (
        EvaluationConfig,
    )

    math_data, _, _ = save_test_data()

    # åˆ›å»º LLM å’Œè¯„ä¼°å™¨ï¼ˆä½¿ç”¨ math æ¨¡æ¿ï¼‰
    print("\n[åˆå§‹åŒ–] åˆ›å»º LLM å’Œè¯„ä¼°å™¨...")
    llm = HelloAgentsLLM(provider="deepseek", model="deepseek-chat")
    eval_config = EvaluationConfig.load_template("math")
    evaluator = UniversalLLMJudgeEvaluator(llm=llm, eval_config=eval_config)
    print(f"âœ“ è¯„ä¼°å™¨å·²åˆå§‹åŒ–ï¼ˆæ¨¡æ¿: mathï¼‰")
    print(f"âœ“ è¯„ä¼°ç»´åº¦: {', '.join(eval_config.get_dimension_names())}")

    # è¯„ä¼°æ•°å­¦é¢˜
    print("\n[è¯„ä¼°] å¼€å§‹è¯„ä¼°æ•°å­¦é¢˜...")
    print("="*70)
    print("ğŸ¯ Math é¢˜ç›®è¯„ä¼°")
    print("="*70)

    all_scores = []

    for i, problem in enumerate(math_data, 1):
        print(f"\nè¯„ä¼°é¢˜ç›® {i}/{len(math_data)}")
        print(f"é¢˜ç›®ID: {problem['id']}")
        print(f"é¢˜ç›®: {problem['problem'][:60]}...")

        result = evaluator.evaluate_single(problem)

        print(f"\nè¯„ä¼°ç»“æœ:")
        for dim, score in result['scores'].items():
            print(f"  {dim}: {score:.1f}/5")
        print(f"  å¹³å‡åˆ†: {result['total_score']:.2f}/5")

        all_scores.append(result)

    # ç»Ÿè®¡
    print("\n" + "="*70)
    print("æ€»ä½“ç»Ÿè®¡")
    print("="*70)

    avg_total = sum(s['total_score'] for s in all_scores) / len(all_scores)
    print(f"\nå¹³å‡æ€»åˆ†: {avg_total:.2f}/5")

    # ä¿å­˜ç»“æœ
    print("\n[ä¿å­˜] ä¿å­˜è¯„ä¼°ç»“æœ...")
    os.makedirs("./evaluation_results", exist_ok=True)
    with open("./evaluation_results/scene_a_math.json", 'w', encoding='utf-8') as f:
        json.dump({
            'scenario': 'A_Math_Standard',
            'template': 'math',
            'field_mapping': 'None (standard fields)',
            'data': math_data,
            'results': all_scores,
            'avg_total_score': avg_total
        }, f, indent=2, ensure_ascii=False)

    print("âœ“ ç»“æœå·²ä¿å­˜åˆ° ./evaluation_results/scene_a_math.json")
    print("\nâœ… åœºæ™¯ A å®Œæˆï¼")


def example_scene_b_code():
    """
    åœºæ™¯ Bï¼šä»£ç è¯„ä¼°

    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨ "code" æ¨¡æ¿
    - éæ ‡å‡†å­—æ®µåï¼ˆcode, expected_outputï¼‰
    - éœ€è¦å­—æ®µæ˜ å°„ï¼šcode â†’ problem, expected_output â†’ answer
    - ç»´åº¦ï¼šcorrectness, robustness, efficiency, readability, style_compliance
    """
    print("\n" + "="*70)
    print("ğŸ“Œ æ¡ˆä¾‹ 1 - åœºæ™¯ Bï¼šä»£ç è¯„ä¼°ï¼ˆå­—æ®µæ˜ å°„ + code æ¨¡æ¿ï¼‰")
    print("="*70)

    from hello_agents import HelloAgentsLLM, UniversalLLMJudgeEvaluator
    from hello_agents.evaluation.benchmarks.data_generation_Universal.evaluation_config import (
        EvaluationConfig,
    )

    _, code_data, _ = save_test_data()

    # åˆ›å»º LLM å’Œè¯„ä¼°å™¨ï¼ˆä½¿ç”¨ code æ¨¡æ¿ + å­—æ®µæ˜ å°„ï¼‰
    print("\n[åˆå§‹åŒ–] åˆ›å»º LLM å’Œè¯„ä¼°å™¨...")
    llm = HelloAgentsLLM(provider="deepseek", model="deepseek-chat")
    eval_config = EvaluationConfig.load_template("code")

    # å®šä¹‰å­—æ®µæ˜ å°„ï¼ˆè‡ªé€‚åº”å¤„ç†ä¸åŒæ•°æ®æ ¼å¼ï¼‰
    field_mapping = {
        "problem": "code",                # æºæ•°æ®ä¸­çš„ "code" å­—æ®µæ˜ å°„åˆ° "problem"
        "answer": "expected_output",      # æºæ•°æ®ä¸­çš„ "expected_output" å­—æ®µæ˜ å°„åˆ° "answer"
    }

    evaluator = UniversalLLMJudgeEvaluator(
        llm=llm,
        eval_config=eval_config,
        field_mapping=field_mapping
    )
    print(f"âœ“ è¯„ä¼°å™¨å·²åˆå§‹åŒ–ï¼ˆæ¨¡æ¿: codeï¼‰")
    print(f"âœ“ è¯„ä¼°ç»´åº¦: {', '.join(eval_config.get_dimension_names())}")
    print(f"âœ“ å­—æ®µæ˜ å°„: {field_mapping}")

    # è¯„ä¼°ä»£ç 
    print("\n[è¯„ä¼°] å¼€å§‹è¯„ä¼°ä»£ç ...")
    print("="*70)
    print("ğŸ¯ Code ä»£ç è¯„ä¼°")
    print("="*70)

    all_scores = []

    for i, problem in enumerate(code_data, 1):
        print(f"\nè¯„ä¼°ä»£ç  {i}/{len(code_data)}")
        print(f"ä»£ç ID: {problem['id']}")
        print(f"ä»£ç : {problem['code'][:40]}...")

        result = evaluator.evaluate_single(problem)

        print(f"\nè¯„ä¼°ç»“æœ:")
        for dim, score in result['scores'].items():
            print(f"  {dim}: {score:.1f}/5")
        print(f"  å¹³å‡åˆ†: {result['total_score']:.2f}/5")

        all_scores.append(result)

    # ç»Ÿè®¡
    print("\n" + "="*70)
    print("æ€»ä½“ç»Ÿè®¡")
    print("="*70)

    avg_total = sum(s['total_score'] for s in all_scores) / len(all_scores)
    print(f"\nå¹³å‡æ€»åˆ†: {avg_total:.2f}/5")

    # ä¿å­˜ç»“æœ
    print("\n[ä¿å­˜] ä¿å­˜è¯„ä¼°ç»“æœ...")
    os.makedirs("./evaluation_results", exist_ok=True)
    with open("./evaluation_results/scene_b_code.json", 'w', encoding='utf-8') as f:
        json.dump({
            'scenario': 'B_Code_FieldMapping',
            'template': 'code',
            'field_mapping': field_mapping,
            'data': code_data,
            'results': all_scores,
            'avg_total_score': avg_total
        }, f, indent=2, ensure_ascii=False)

    print("âœ“ ç»“æœå·²ä¿å­˜åˆ° ./evaluation_results/scene_b_code.json")
    print("\nâœ… åœºæ™¯ B å®Œæˆï¼")


def example_scene_c_writing():
    """
    åœºæ™¯ Cï¼šå†™ä½œè¯„ä¼°

    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨ "writing" æ¨¡æ¿
    - éæ ‡å‡†å­—æ®µåï¼ˆcontent, targetï¼‰
    - éœ€è¦å­—æ®µæ˜ å°„ï¼šcontent â†’ problem, target â†’ answer
    - ç»´åº¦ï¼šaccuracy, coherence, richness, creativity_style, engagement
    """
    print("\n" + "="*70)
    print("ğŸ“Œ æ¡ˆä¾‹ 1 - åœºæ™¯ Cï¼šå†™ä½œè¯„ä¼°ï¼ˆå­—æ®µæ˜ å°„ + writing æ¨¡æ¿ï¼‰")
    print("="*70)

    from hello_agents import HelloAgentsLLM, UniversalLLMJudgeEvaluator
    from hello_agents.evaluation.benchmarks.data_generation_Universal.evaluation_config import (
        EvaluationConfig,
    )

    _, _, writing_data = save_test_data()

    # åˆ›å»º LLM å’Œè¯„ä¼°å™¨ï¼ˆä½¿ç”¨ writing æ¨¡æ¿ + å­—æ®µæ˜ å°„ï¼‰
    print("\n[åˆå§‹åŒ–] åˆ›å»º LLM å’Œè¯„ä¼°å™¨...")
    llm = HelloAgentsLLM(provider="deepseek", model="deepseek-chat")
    eval_config = EvaluationConfig.load_template("writing")

    # å®šä¹‰å­—æ®µæ˜ å°„ï¼ˆè‡ªé€‚åº”å¤„ç†ä¸åŒæ•°æ®æ ¼å¼ï¼‰
    field_mapping = {
        "problem": "content",           # æºæ•°æ®ä¸­çš„ "content" å­—æ®µæ˜ å°„åˆ° "problem"
        "answer": "target",             # æºæ•°æ®ä¸­çš„ "target" å­—æ®µæ˜ å°„åˆ° "answer"
    }

    evaluator = UniversalLLMJudgeEvaluator(
        llm=llm,
        eval_config=eval_config,
        field_mapping=field_mapping
    )
    print(f"âœ“ è¯„ä¼°å™¨å·²åˆå§‹åŒ–ï¼ˆæ¨¡æ¿: writingï¼‰")
    print(f"âœ“ è¯„ä¼°ç»´åº¦: {', '.join(eval_config.get_dimension_names())}")
    print(f"âœ“ å­—æ®µæ˜ å°„: {field_mapping}")

    # è¯„ä¼°å†™ä½œ
    print("\n[è¯„ä¼°] å¼€å§‹è¯„ä¼°å†™ä½œ...")
    print("="*70)
    print("ğŸ¯ Writing å†™ä½œè¯„ä¼°")
    print("="*70)

    all_scores = []

    for i, problem in enumerate(writing_data, 1):
        print(f"\nè¯„ä¼°å†™ä½œ {i}/{len(writing_data)}")
        print(f"å†™ä½œID: {problem['id']}")
        print(f"å†…å®¹: {problem['content'][:40]}...")

        result = evaluator.evaluate_single(problem)

        print(f"\nè¯„ä¼°ç»“æœ:")
        for dim, score in result['scores'].items():
            print(f"  {dim}: {score:.1f}/5")
        print(f"  å¹³å‡åˆ†: {result['total_score']:.2f}/5")

        all_scores.append(result)

    # ç»Ÿè®¡
    print("\n" + "="*70)
    print("æ€»ä½“ç»Ÿè®¡")
    print("="*70)

    avg_total = sum(s['total_score'] for s in all_scores) / len(all_scores)
    print(f"\nå¹³å‡æ€»åˆ†: {avg_total:.2f}/5")

    # ä¿å­˜ç»“æœ
    print("\n[ä¿å­˜] ä¿å­˜è¯„ä¼°ç»“æœ...")
    os.makedirs("./evaluation_results", exist_ok=True)
    with open("./evaluation_results/scene_c_writing.json", 'w', encoding='utf-8') as f:
        json.dump({
            'scenario': 'C_Writing_FieldMapping',
            'template': 'writing',
            'field_mapping': field_mapping,
            'data': writing_data,
            'results': all_scores,
            'avg_total_score': avg_total
        }, f, indent=2, ensure_ascii=False)

    print("âœ“ ç»“æœå·²ä¿å­˜åˆ° ./evaluation_results/scene_c_writing.json")
    print("\nâœ… åœºæ™¯ C å®Œæˆï¼")


# ============================================================================
# æ¡ˆä¾‹ 2ï¼šä½¿ç”¨é«˜çº§å·¥å…·ï¼ˆä¸€è¡Œæå®šï¼‰
# ============================================================================


def example_high_level_tool():
    """
    æ¡ˆä¾‹ 2ï¼šä½¿ç”¨é«˜çº§å·¥å…·

    ä¼˜ç‚¹ï¼š
    - ä¸€è¡Œä»£ç æå®šå®Œæ•´æµç¨‹
    - è‡ªåŠ¨å¤„ç†æ•°æ®åŠ è½½ã€å­—æ®µæ˜ å°„ã€æŠ¥å‘Šç”Ÿæˆ
    - æ”¯æŒå¤šæ¨¡æ¿å’Œå¤šæ•°æ®æº
    """
    print("\n" + "="*70)
    print("ğŸ“Œ æ¡ˆä¾‹ 2ï¼šä½¿ç”¨é«˜çº§å·¥å…·ï¼ˆUniversalLLMJudgeToolï¼‰")
    print("="*70)

    from hello_agents import HelloAgentsLLM, UniversalLLMJudgeTool

    math_data, _, _ = save_test_data()

    print("\n[åˆå§‹åŒ–] åˆ›å»º LLM å’Œå·¥å…·...")
    llm = HelloAgentsLLM(provider="deepseek", model="deepseek-chat")
    tool = UniversalLLMJudgeTool(template="math", llm=llm)
    print("âœ“ å·¥å…·å·²åˆå§‹åŒ–")

    print("\n[è¿è¡Œ] è°ƒç”¨ run() æ–¹æ³•è¿›è¡Œå®Œæ•´è¯„ä¼°...")

    # ä¿å­˜æ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶
    temp_data_path = "./test_data/temp_math_data.json"
    os.makedirs(os.path.dirname(temp_data_path), exist_ok=True)
    with open(temp_data_path, 'w', encoding='utf-8') as f:
        json.dump(math_data, f, indent=2, ensure_ascii=False)

    result = tool.run({
        "source_config": {"path": temp_data_path},
        "field_mapping": {"problem": "problem", "answer": "answer"},
        "template": "math",
        "max_samples": 2,
        "output_dir": "evaluation_results/case2_high_level"
    })

    print("\n" + "="*70)
    print("è¯„ä¼°å®Œæˆï¼ˆé«˜çº§å·¥å…·è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Šå’Œç»Ÿè®¡ï¼‰")
    print("="*70)
    print(f"\nè¯„ä¼°ç»“æœ:\n{result}")

    print("\nâœ… æ¡ˆä¾‹ 2 å®Œæˆï¼")


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================


if __name__ == "__main__":
    print("\n" + "="*70)
    print("ğŸ¯ LLM Judge é€šç”¨è¯„ä¼°å™¨ - çœŸå®åœºæ™¯ä¸¤å±‚çº§å¯¹æ¯”æµ‹è¯•")
    print("="*70)

    # print("\næœ¬è„šæœ¬æ¼”ç¤ºé€šç”¨è¯„ä¼°å™¨åœ¨çœŸå®åœºæ™¯ä¸­çš„åº”ç”¨ï¼š")
    # print("\nğŸ“Œ æ¡ˆä¾‹ 1ï¼šç›´æ¥ä½¿ç”¨è¯„ä¼°å™¨ï¼ˆçµæ´»ã€é€æ˜ï¼‰")
    # print("  åœºæ™¯ Aï¼šæ•°å­¦é¢˜ - æ ‡å‡†å­—æ®µ + math æ¨¡æ¿")
    # print("  åœºæ™¯ Bï¼šä»£ç    - å­—æ®µæ˜ å°„ + code æ¨¡æ¿")
    # print("  åœºæ™¯ Cï¼šå†™ä½œ   - å­—æ®µæ˜ å°„ + writing æ¨¡æ¿")
    # print("\nğŸ“Œ æ¡ˆä¾‹ 2ï¼šä½¿ç”¨é«˜çº§å·¥å…·ï¼ˆç®€æ´ã€è‡ªåŠ¨åŒ–ï¼‰")

    # è¿è¡Œæ¡ˆä¾‹ 1 çš„ä¸‰ä¸ªåœºæ™¯
    print("\n" + "="*70)
    print("å¼€å§‹æ¡ˆä¾‹ 1ï¼šç›´æ¥ä½¿ç”¨è¯„ä¼°å™¨")
    print("="*70)

    example_scene_a_math()
    # example_scene_b_code()      # å–æ¶ˆæ³¨é‡Šæ¥è¿è¡Œåœºæ™¯ B
    # example_scene_c_writing()   # å–æ¶ˆæ³¨é‡Šæ¥è¿è¡Œåœºæ™¯ C

    # è¿è¡Œæ¡ˆä¾‹ 2
    print("\n" + "="*70)
    print("å¼€å§‹æ¡ˆä¾‹ 2ï¼šä½¿ç”¨é«˜çº§å·¥å…·")
    print("="*70)
    # example_high_level_tool()   # å–æ¶ˆæ³¨é‡Šæ¥è¿è¡Œæ¡ˆä¾‹ 2

    # æ€»ç»“
    # print("\n" + "="*70)
    # print("ğŸ“Š å…³é”®è¦ç‚¹æ€»ç»“")
    # print("="*70)

    # print("\nğŸ”‘ é€šç”¨è¯„ä¼°å™¨çš„æ ¸å¿ƒèƒ½åŠ›ï¼š")
    # print("\n1. å¤šæ¨¡æ¿æ”¯æŒï¼ˆFlexibleï¼‰")
    # print("   âœ“ mathï¼šæ•°å­¦é¢˜ - é‡è§†æ­£ç¡®æ€§ã€éš¾åº¦åŒ¹é…ã€åˆ›æ„è§£æ³•")
    # print("   âœ“ codeï¼šä»£ç    - é‡è§†é²æ£’æ€§ã€æ•ˆç‡ã€ä»£ç é£æ ¼")
    # print("   âœ“ writingï¼šå†™ä½œ - é‡è§†å‡†ç¡®æ€§ã€è¿è´¯æ€§ã€åˆ›æ„è¡¨è¾¾")
    # print("   âœ“ qaï¼šé—®ç­”   - é‡è§†å‡†ç¡®æ€§ã€å®Œæ•´æ€§ã€æœ‰ç”¨æ€§")

    # print("\n2. å­—æ®µæ˜ å°„ï¼ˆAdaptiveï¼‰")
    # print("   âœ“ è‡ªåŠ¨å¤„ç†ä¸åŒæ•°æ®æ ¼å¼")
    # print("   âœ“ æ”¯æŒéæ ‡å‡†å­—æ®µå")
    # print("   âœ“ æ— éœ€ä¿®æ”¹æºæ•°æ®")

    # print("\n3. ä¸¤å±‚çº§ä½¿ç”¨ï¼ˆFlexible Hierarchyï¼‰")
    # print("   ä½å±‚çº§ï¼šUniversalLLMJudgeEvaluator - çµæ´»å®šåˆ¶")
    # print("   é«˜å±‚çº§ï¼šUniversalLLMJudgeTool - å¿«é€Ÿéƒ¨ç½²")

    # print("\nâœ… æ ¹æ®ä½ çš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„ä½¿ç”¨æ–¹å¼ï¼")
