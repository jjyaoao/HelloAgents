# UniversalLLMJudge & UniversalWinRate ä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

HelloAgents æä¾›ä¸¤ä¸ªå¼ºå¤§çš„é€šç”¨è¯„ä¼°æ¨¡å—ï¼Œæ”¯æŒå¤šç§å†…å®¹ç±»å‹çš„æ™ºèƒ½è´¨é‡è¯„ä¼°ï¼š

- **UniversalLLMJudge**: ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„å§”ï¼Œå¯¹å•ä¸ªé¡¹ç›®è¿›è¡Œå¤šç»´åº¦è´¨é‡è¯„ä¼°
- **UniversalWinRate**: é€šè¿‡æˆå¯¹å¯¹æ¯”çš„æ–¹å¼ï¼Œè®¡ç®—ç”Ÿæˆæ•°æ®ç›¸å¯¹äºå‚è€ƒæ•°æ®çš„èƒœç‡

### æ ¸å¿ƒç‰¹æ€§

- **ä¸¤å±‚çº§æ¥å£è®¾è®¡**:
  - **ä½å±‚çº§API**: æä¾›å®Œå…¨çš„å®šåˆ¶èƒ½åŠ›å’Œæ§åˆ¶æƒ
  - **é«˜å±‚çº§API**: æä¾›ä¸€é”®å¼çš„å¿«é€Ÿè¯„ä¼°ä½“éªŒ
- **å¤šæ¨¡æ¿æ”¯æŒ**: å†…ç½®æ•°å­¦ã€ä»£ç ã€å†™ä½œã€é—®ç­”å››ç§è¯„ä¼°æ¨¡æ¿
- **è‡ªå®šä¹‰ç»´åº¦**: æ”¯æŒåˆ›å»ºä¸“é—¨çš„è¯„ä¼°ç»´åº¦å’Œæ¨¡æ¿
- **æ™ºèƒ½å­—æ®µæ˜ å°„**: è‡ªåŠ¨é€‚é…ä¸åŒæ•°æ®æ ¼å¼
- **æ‰¹é‡å¤„ç†**: é«˜æ•ˆå¤„ç†å¤§è§„æ¨¡æ•°æ®è¯„ä¼°

## å¿«é€Ÿå¼€å§‹

###  UniversalLLMJudge

```python
from hello_agents import HelloAgentsLLM, UniversalLLMJudgeTool

# 1. åˆå§‹åŒ– LLM å’Œè¯„ä¼°å·¥å…·
llm = HelloAgentsLLM(provider="deepseek", model="deepseek-chat")
tool = UniversalLLMJudgeTool(template="math", llm=llm)

# 2. å‡†å¤‡æ•°æ®æ–‡ä»¶ data.json
# [
#   {"id": "001", "problem": "è§£æ–¹ç¨‹: 2x + 5 = 15", "answer": "x = 5"},
#   {"id": "002", "problem": "è®¡ç®—: âˆš144", "answer": "12"}
# ]

# 3. ä¸€è¡Œä»£ç å®Œæˆè¯„ä¼°
result = tool.run({
    "source_config": {"path": "data.json"},
    "template": "math",
    "output_dir": "results"
})

print(f"å¹³å‡åˆ†: {result['summary']['average_score']}")
```

###  UniversalWinRate

```python
from hello_agents import HelloAgentsLLM, UniversalWinRateEvaluator
from hello_agents.evaluation.benchmarks.data_generation_Universal.evaluation_config import EvaluationConfig

# 1. åˆå§‹åŒ–
llm = HelloAgentsLLM(provider="deepseek", model="deepseek-chat")
config = EvaluationConfig.load_template("writing")
evaluator = UniversalWinRateEvaluator(llm=llm, eval_config=config)

# 2. å‡†å¤‡å¯¹æ¯”æ•°æ®
generated = [{"id": "g1", "problem": "æè¿°æ˜¥å¤©", "answer": "æ˜¥å¤©æ¥äº†ï¼ŒèŠ±å„¿å¼€äº†"}]
reference = [{"id": "r1", "problem": "æè¿°æ˜¥å¤©", "answer": "æ˜¥æš–èŠ±å¼€ï¼Œä¸‡ç‰©å¤è‹"}]

# 3. è®¡ç®—èƒœç‡
result = evaluator.evaluate_win_rate(generated, reference, num_comparisons=5)
print(f"ç”Ÿæˆæ•°æ®èƒœç‡: {result['metrics']['win_rate']:.2%}")
```

## è¯„ä¼°æ¨¡æ¿

| æ¨¡æ¿ | ç”¨é€” | è¯„ä¼°ç»´åº¦ |
|------|------|--------|
| **math** | æ•°å­¦é¢˜ | correctness, clarity, completeness, difficulty_match, originality |
| **code** | ä»£ç  | correctness, robustness, efficiency, readability, style_compliance |
| **writing** | å†™ä½œ | accuracy, coherence, richness, creativity_style, engagement |
| **qa** | é—®ç­” | correctness, clarity, completeness, helpfulness |

## è‡ªå®šä¹‰ç»´åº¦è¯¦è§£

### åˆ›å»ºè‡ªå®šä¹‰è¯„ä¼°ç»´åº¦

å½“å†…ç½®æ¨¡æ¿æ— æ³•æ»¡è¶³æ‚¨çš„ç‰¹å®šéœ€æ±‚æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ `EvaluationConfig.custom()` åˆ›å»ºå®Œå…¨è‡ªå®šä¹‰çš„è¯„ä¼°ç»´åº¦ï¼š

```python
from hello_agents.evaluation.benchmarks.data_generation_Universal.evaluation_config import EvaluationConfig
from hello_agents import HelloAgentsLLM, UniversalLLMJudgeEvaluator

# åˆ›å»ºè‡ªå®šä¹‰è¯„ä¼°é…ç½®
custom_config = EvaluationConfig.custom(
    technical_accuracy="æŠ€æœ¯å®ç°çš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§",
    innovation_solution="è§£å†³æ–¹æ¡ˆçš„åˆ›æ–°æ€§å’Œç‹¬ç‰¹æ€§",
    practical_value="å®é™…åº”ç”¨ä»·å€¼å’Œå¯æ“ä½œæ€§",
    presentation_clarity="è¡¨è¿°çš„æ¸…æ™°åº¦å’Œé€»è¾‘æ€§"
)

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
llm = HelloAgentsLLM(provider="deepseek", model="deepseek-chat")
evaluator = UniversalLLMJudgeEvaluator(llm=llm, eval_config=custom_config)

# è¯„ä¼°æ•°æ®
data = {
    "id": "proj_001",
    "problem": "è®¾è®¡ä¸€ä¸ªæ™ºèƒ½å®¶å±…æ§åˆ¶ç³»ç»Ÿ",
    "answer": "åŸºäºIoTå’Œè¾¹ç¼˜è®¡ç®—çš„åˆ†å¸ƒå¼æ¶æ„..."
}
result = evaluator.evaluate_single(data)

print(f"æŠ€æœ¯åˆ›æ–°æ€§: {result['scores']['innovation_solution']}/5")
print(f"å®é™…ä»·å€¼: {result['scores']['practical_value']}/5")
```

### è‡ªå®šä¹‰ç»´åº¦çš„è®¾è®¡åŸåˆ™

1. **æ˜ç¡®å…·ä½“**: æ¯ä¸ªç»´åº¦åº”è¯¥æœ‰æ˜ç¡®çš„è¯„ä¼°æ ‡å‡†
2. **ç›¸äº’ç‹¬ç«‹**: é¿å…ç»´åº¦ä¹‹é—´çš„é‡å å’Œäº¤å‰
3. **å¯è¡¡é‡æ€§**: LLMèƒ½å¤Ÿç†è§£å’Œè¯„ä¼°çš„æ ‡å‡†
4. **ä¸šåŠ¡ç›¸å…³æ€§**: ä¸æ‚¨çš„å…·ä½“åº”ç”¨åœºæ™¯ç´§å¯†ç›¸å…³

### ä¸åŒåœºæ™¯çš„è‡ªå®šä¹‰ç»´åº¦ç¤ºä¾‹

#### è½¯ä»¶è®¾è®¡è¯„å®¡
```python
software_design_config = EvaluationConfig.custom(
    architecture_quality="ç³»ç»Ÿæ¶æ„è®¾è®¡çš„åˆç†æ€§å’Œæ‰©å±•æ€§",
    security_consideration="å®‰å…¨è€ƒè™‘å’Œé£é™©é˜²æ§æªæ–½",
    scalability="ç³»ç»Ÿçš„å¯æ‰©å±•æ€§å’Œæ€§èƒ½è§„åˆ’",
    maintainability="ä»£ç çš„å¯ç»´æŠ¤æ€§å’Œæ–‡æ¡£å®Œæ•´æ€§"
)
```

#### å•†ä¸šè®¡åˆ’è¯„ä¼°
```python
business_plan_config = EvaluationConfig.custom(
    market_analysis="å¸‚åœºåˆ†æçš„æ·±åº¦å’Œå‡†ç¡®æ€§",
    financial_model="è´¢åŠ¡æ¨¡å‹çš„åˆç†æ€§å’Œå¯è¡Œæ€§",
    competitive_advantage="ç«äº‰ä¼˜åŠ¿çš„æ˜ç¡®æ€§å’ŒæŒç»­æ€§",
    team_capability="å›¢é˜ŸèƒŒæ™¯å’Œæ‰§è¡Œèƒ½åŠ›çš„åŒ¹é…åº¦"
)
```


### ç»´åº¦æƒé‡è‡ªå®šä¹‰

è™½ç„¶é»˜è®¤æ‰€æœ‰ç»´åº¦æƒé‡ç›¸ç­‰ï¼Œä½†æ‚¨å¯ä»¥é€šè¿‡ç»“æœåå¤„ç†å®ç°è‡ªå®šä¹‰æƒé‡ï¼š

```python
def apply_weighted_scores(result, weights):
    """åº”ç”¨è‡ªå®šä¹‰æƒé‡åˆ°è¯„ä¼°ç»“æœ"""
    weighted_total = 0
    total_weight = 0

    for dimension, weight in weights.items():
        if dimension in result['scores']:
            weighted_total += result['scores'][dimension] * weight
            total_weight += weight

    result['weighted_total_score'] = weighted_total / total_weight if total_weight > 0 else 0
    return result

# ä½¿ç”¨æƒé‡: æŠ€æœ¯å‡†ç¡®æ€§40%, åˆ›æ–°æ€§35%, å®ç”¨ä»·å€¼25%
weights = {
    'technical_accuracy': 0.4,
    'innovation_solution': 0.35,
    'practical_value': 0.25
}

result = evaluator.evaluate_single(data)
weighted_result = apply_weighted_scores(result, weights)
print(f"åŠ æƒæ€»åˆ†: {weighted_result['weighted_total_score']}")
```

## æ¨¡æ¿ç³»ç»Ÿæ·±å…¥åˆ†æ

### å„æ¨¡æ¿è¯¦ç»†å¯¹æ¯”

#### Math æ¨¡æ¿ - æ•°å­¦é—®é¢˜è¯„ä¼°

**è¯„ä¼°ç»´åº¦**:
- `correctness`: è§£ç­”çš„æ•°å­¦æ­£ç¡®æ€§
- `clarity`: è§£ç­”è¿‡ç¨‹çš„æ¸…æ™°åº¦å’Œé€»è¾‘æ€§
- `completeness`: è§£ç­”çš„å®Œæ•´æ€§å’Œæ­¥éª¤çš„è¯¦å°½ç¨‹åº¦
- `difficulty_match`: è§£ç­”éš¾åº¦ä¸é¢˜ç›®éš¾åº¦çš„åŒ¹é…åº¦
- `originality`: è§£æ³•çš„æ–°é¢–æ€§å’Œåˆ›é€ æ€§


**ä¸»è¦å·®å¼‚**:
å¼ºè°ƒé€»è¾‘æ¨ç†çš„å‡†ç¡®æ€§å’Œæ­¥éª¤çš„å®Œæ•´æ€§ï¼Œç‰¹åˆ«å…³æ³¨æ•°å­¦ç¬¦å·å’Œå…¬å¼çš„æ­£ç¡®ä½¿ç”¨ã€‚

#### Code æ¨¡æ¿ - ä»£ç è´¨é‡è¯„ä¼°

**è¯„ä¼°ç»´åº¦**:
- `correctness`: ä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§å’Œbugæ•°é‡
- `robustness`: ä»£ç çš„å¥å£®æ€§å’Œå¼‚å¸¸å¤„ç†èƒ½åŠ›
- `efficiency`: ç®—æ³•æ•ˆç‡å’Œèµ„æºæ¶ˆè€—
- `readability`: ä»£ç çš„å¯è¯»æ€§å’Œå‘½åè§„èŒƒ
- `style_compliance`: ç¼–ç è§„èŒƒçš„éµå¾ªç¨‹åº¦


**ä¸»è¦å·®å¼‚**:
é‡ç‚¹å…³æ³¨æŠ€æœ¯å®ç°è´¨é‡ï¼ŒåŒ…æ‹¬æ€§èƒ½ã€å®‰å…¨æ€§å’Œå¯ç»´æŠ¤æ€§ç­‰å·¥ç¨‹åŒ–æŒ‡æ ‡ã€‚

#### Writing æ¨¡æ¿ - å†™ä½œè´¨é‡è¯„ä¼°

**è¯„ä¼°ç»´åº¦**:
- `accuracy`: å†…å®¹çš„äº‹å®å‡†ç¡®æ€§å’Œä¿¡æ¯å¯é æ€§
- `coherence`: æ–‡ç« çš„é€»è¾‘è¿è´¯æ€§å’Œç»“æ„æ¸…æ™°åº¦
- `richness`: è¯æ±‡ä¸°å¯Œæ€§å’Œè¡¨è¾¾å¤šæ ·æ€§
- `creativity_style`: åˆ›æ„æ€§å’Œä¸ªäººé£æ ¼ç‰¹è‰²
- `engagement`: å†…å®¹çš„å¸å¼•åŠ›å’Œè¯»è€…å‚ä¸åº¦


**ä¸»è¦å·®å¼‚**:
æ³¨é‡è¯­è¨€è¡¨è¾¾çš„è‰ºæœ¯æ€§å’Œæ„ŸæŸ“åŠ›ï¼Œå…³æ³¨è¯»è€…çš„ä¸»è§‚æ„Ÿå—å’Œä½“éªŒã€‚

#### QA æ¨¡æ¿ - é—®ç­”è´¨é‡è¯„ä¼°

**è¯„ä¼°ç»´åº¦**:
- `correctness`: å›ç­”çš„äº‹å®å‡†ç¡®æ€§
- `clarity`: å›ç­”çš„æ¸…æ™°åº¦å’Œæ˜“ç†è§£æ€§
- `completeness`: å›ç­”çš„å®Œæ•´æ€§å’Œè¦†ç›–é¢
- `helpfulness`: å›ç­”çš„å®ç”¨ä»·å€¼å’Œå¸®åŠ©ç¨‹åº¦


**ä¸»è¦å·®å¼‚**:
å¼ºè°ƒå›ç­”çš„å®ç”¨æ€§å’Œé’ˆå¯¹æ€§ï¼Œå…³æ³¨æ˜¯å¦çœŸæ­£è§£å†³äº†ç”¨æˆ·çš„é—®é¢˜ã€‚


### æ¨¡æ¿å¯¹æ¯”å®éªŒ

åŒä¸€ä»½æ•°æ®ä½¿ç”¨ä¸åŒæ¨¡æ¿ä¼šäº§ç”Ÿæ˜¾è‘—ä¸åŒçš„è¯„ä¼°ç»“æœï¼š

```python
from hello_agents import HelloAgentsLLM, UniversalLLMJudgeEvaluator
from hello_agents.evaluation.benchmarks.data_generation_Universal.evaluation_config import EvaluationConfig

llm = HelloAgentsLLM(provider="deepseek", model="deepseek-chat")
data = {
    "id": "example",
    "problem": "ç”¨Pythonå®ç°ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•",
    "answer": """
def quicksort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quicksort(left) + middle + quicksort(right)
    """
}

# ä½¿ç”¨ä¸åŒæ¨¡æ¿è¯„ä¼°åŒä¸€ä»½ä»£ç 
templates = ["code", "math", "writing", "qa"]
results = {}

for template in templates:
    config = EvaluationConfig.load_template(template)
    evaluator = UniversalLLMJudgeEvaluator(llm=llm, eval_config=config)
    result = evaluator.evaluate_single(data)
    results[template] = result['total_score']

    print(f"{template}æ¨¡æ¿è¯„åˆ†: {result['total_score']:.2f}")
    print(f"  ç»´åº¦åˆ†æ•°: {result['scores']}")

# è¾“å‡ºç¤ºä¾‹:
# codeæ¨¡æ¿è¯„åˆ†: 4.20  (ä¾§é‡æŠ€æœ¯å®ç°)
# mathæ¨¡æ¿è¯„åˆ†: 3.80  (ä¾§é‡ç®—æ³•é€»è¾‘)
# writingæ¨¡æ¿è¯„åˆ†: 3.50  (ä¾§é‡è¡¨è¾¾æ¸…æ™°)
# qaæ¨¡æ¿è¯„åˆ†: 3.90  (ä¾§é‡é—®é¢˜è§£å†³)
```

**å…³é”®è§‚å¯Ÿ**:
- **Codeæ¨¡æ¿**ç»™å‡ºæœ€é«˜åˆ†ï¼Œå› ä¸ºå®ƒä¸“é—¨è¯„ä¼°ä»£ç å®ç°è´¨é‡
- **Mathæ¨¡æ¿**å…³æ³¨ç®—æ³•çš„é€»è¾‘æ­£ç¡®æ€§ï¼Œå¯¹å®ç°ç»†èŠ‚ä¸å¤ªæ•æ„Ÿ
- **Writingæ¨¡æ¿**æ›´å…³æ³¨ä»£ç çš„å¯è¯»æ€§å’Œæ–‡æ¡£ä»·å€¼
- **QAæ¨¡æ¿**è¯„ä¼°çš„æ˜¯è¿™ä¸ªå›ç­”æ˜¯å¦å¾ˆå¥½åœ°è§£å†³äº†"å®ç°å¿«é€Ÿæ’åº"è¿™ä¸ªé—®é¢˜

## è‡ªå®šä¹‰æ¨¡æ¿åˆ›å»º

### æ‰©å±•ç°æœ‰æ¨¡æ¿

å½“å†…ç½®æ¨¡æ¿ä¸æ‚¨çš„éœ€æ±‚æ¥è¿‘ä½†éœ€è¦è°ƒæ•´æ—¶ï¼Œå¯ä»¥é€šè¿‡ç»§æ‰¿å’Œæ‰©å±•æ¥åˆ›å»ºæ–°æ¨¡æ¿ï¼š

```python
from hello_agents.evaluation.benchmarks.data_generation_Universal.evaluation_config import EvaluationConfig

# åŸºäºcodeæ¨¡æ¿æ‰©å±•ï¼Œæ·»åŠ AIä»£ç ç‰¹å®šç»´åº¦
ai_code_config = EvaluationConfig.custom(
    correctness="ä»£ç åŠŸèƒ½æ­£ç¡®æ€§",
    robustness="ä»£ç å¥å£®æ€§å’Œå¼‚å¸¸å¤„ç†",
    efficiency="ç®—æ³•æ•ˆç‡å’Œæ€§èƒ½è¡¨ç°",
    readability="ä»£ç å¯è¯»æ€§å’Œå‘½åè§„èŒƒ",
    style_compliance="ç¼–ç è§„èŒƒéµå¾ªç¨‹åº¦",
    ai_safety="AIå®‰å…¨æ€§å’Œä¼¦ç†è€ƒè™‘",
    model_efficiency="æ¨¡å‹æ¨ç†æ•ˆç‡å’Œèµ„æºæ¶ˆè€—"
)

# åŸºäºwritingæ¨¡æ¿æ‰©å±•ï¼Œä¸“é—¨ç”¨äºæŠ€æœ¯æ–‡æ¡£
tech_writing_config = EvaluationConfig.custom(
    accuracy="æŠ€æœ¯ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œæ—¶æ•ˆæ€§",
    coherence="æ–‡æ¡£ç»“æ„é€»è¾‘æ€§å’Œæ¡ç†æ€§",
    richness="æŠ€æœ¯æœ¯è¯­ä½¿ç”¨çš„å‡†ç¡®æ€§å’Œä¸°å¯Œæ€§",
    creativity_style="æŠ€æœ¯è¡¨è¾¾çš„åˆ›æ–°æ€§å’Œé£æ ¼",
    engagement="æ–‡æ¡£çš„å¸å¼•åŠ›å’Œæ˜“è¯»æ€§",
    technical_depth="æŠ€æœ¯æ·±åº¦å’Œä¸“ä¸šæ€§",
    practical_guidance="å®æ“æŒ‡å¯¼çš„ä»·å€¼å’Œå¯è¡Œæ€§"
)
```

### åˆ›å»ºé¢†åŸŸä¸“ç”¨æ¨¡æ¿

#### åŒ»ç–—é¢†åŸŸè¯„ä¼°æ¨¡æ¿

```python
medical_config = EvaluationConfig.custom(
    diagnostic_accuracy="è¯Šæ–­å‡†ç¡®æ€§å’ŒåŒ»å­¦ä¾æ®",
    treatment_appropriateness="æ²»ç–—æ–¹æ¡ˆåˆç†æ€§å’Œå®‰å…¨æ€§",
    patient_safety="æ‚£è€…å®‰å…¨è€ƒè™‘å’Œé£é™©è¯„ä¼°",
    clinical_evidence="ä¸´åºŠè¯æ®æ”¯æŒå’Œç§‘å­¦æ€§",
    ethical_compliance="åŒ»å­¦ä¼¦ç†å’Œåˆè§„æ€§",
    clarity="åŒ»å­¦è¡¨è¿°çš„æ¸…æ™°åº¦å’Œä¸“ä¸šæ€§"
)

# ä½¿ç”¨åŒ»ç–—æ¨¡æ¿è¯„ä¼°ç—…ä¾‹åˆ†æ
from hello_agents import HelloAgentsLLM, UniversalLLMJudgeEvaluator

llm = HelloAgentsLLM(provider="deepseek", model="deepseek-chat")
medical_evaluator = UniversalLLMJudgeEvaluator(llm=llm, eval_config=medical_config)

case_data = {
    "id": "case_001",
    "problem": "æ‚£è€…ï¼Œç”·ï¼Œ65å²ï¼Œèƒ¸ç—›3å°æ—¶ï¼Œå¿ƒç”µå›¾ç¤ºSTæ®µæŠ¬é«˜",
    "answer": "åˆæ­¥è¯Šæ–­ä¸ºæ€¥æ€§å¿ƒè‚Œæ¢—æ­»ï¼Œå»ºè®®ç«‹å³è¿›è¡Œå† è„‰é€ å½±å¹¶å‡†å¤‡PCIæ²»ç–—"
}

result = medical_evaluator.evaluate_single(case_data)
print(f"è¯Šæ–­å‡†ç¡®æ€§: {result['scores']['diagnostic_accuracy']}/5")
print(f"æ²»ç–—æ–¹æ¡ˆåˆç†æ€§: {result['scores']['treatment_appropriateness']}/5")
```

#### æ³•å¾‹é¢†åŸŸè¯„ä¼°æ¨¡æ¿

```python
legal_config = EvaluationConfig.custom(
    legal_accuracy="æ³•å¾‹æ¡æ–‡å¼•ç”¨çš„å‡†ç¡®æ€§",
    reasoning_logic="æ³•å¾‹æ¨ç†çš„é€»è¾‘ä¸¥å¯†æ€§",
    evidence_analysis="è¯æ®åˆ†æçš„å……åˆ†æ€§",
    argumentation_strengthè®ºè¯çš„è¯´æœåŠ›å’Œå¼ºåº¦",
    compliance="æ³•å¾‹ç¨‹åºåˆè§„æ€§",
    practical_feasibility="æ³•å¾‹å»ºè®®çš„å¯è¡Œæ€§"
)
```

#### æ•™è‚²é¢†åŸŸè¯„ä¼°æ¨¡æ¿

```python
education_config = EvaluationConfig.custom(
    learning_objectives_alignment="ä¸å­¦ä¹ ç›®æ ‡çš„ä¸€è‡´æ€§",
    content_accuracy="çŸ¥è¯†å†…å®¹çš„å‡†ç¡®æ€§",
    pedagogical_effectiveness="æ•™å­¦æ–¹æ³•å’Œç­–ç•¥çš„æœ‰æ•ˆæ€§",
    student_engagement="å­¦ç”Ÿå‚ä¸åº¦å’Œäº’åŠ¨æ€§",
    assessment_alignment="è¯„ä¼°æ–¹å¼ä¸ç›®æ ‡çš„ä¸€è‡´æ€§",
    accessibility="å†…å®¹çš„å¯è®¿é—®æ€§å’ŒåŒ…å®¹æ€§"
)
```


### æ¨¡æ¿å…±äº«å’Œå¤ç”¨

```python
# ä¿å­˜è‡ªå®šä¹‰æ¨¡æ¿é…ç½®
def save_custom_template(config, template_name, save_path="custom_templates.json"):
    """ä¿å­˜è‡ªå®šä¹‰æ¨¡æ¿åˆ°æ–‡ä»¶"""
    import json

    templates = {}
    try:
        with open(save_path, 'r', encoding='utf-8') as f:
            templates = json.load(f)
    except FileNotFoundError:
        pass

    templates[template_name] = {
        'dimensions': config.dimensions,
        'description': f"Custom template: {template_name}"
    }

    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(templates, f, ensure_ascii=False, indent=2)

# åŠ è½½è‡ªå®šä¹‰æ¨¡æ¿
def load_custom_template(template_name, template_path="custom_templates.json"):
    """ä»æ–‡ä»¶åŠ è½½è‡ªå®šä¹‰æ¨¡æ¿"""
    import json

    with open(template_path, 'r', encoding='utf-8') as f:
        templates = json.load(f)

    if template_name not in templates:
        raise ValueError(f"Template '{template_name}' not found")

    template_data = templates[template_name]
    return EvaluationConfig.custom(**template_data['dimensions'])

# ä½¿ç”¨ç¤ºä¾‹
save_custom_template(medical_config, "medical_evaluation")
loaded_config = load_custom_template("medical_evaluation")
```

### æ ‡å‡†å­—æ®µå

```python
{
    "id": "item_001",      # é¡¹ç›® ID
    "problem": "...",      # é¢˜ç›®/é—®é¢˜/å†…å®¹
    "answer": "..."        # ç­”æ¡ˆ/è§£ç­”/ç›®æ ‡
}
```

### å­—æ®µæ˜ å°„

å½“æ•°æ®æ ¼å¼ä¸åŒæ—¶ï¼Œä½¿ç”¨å­—æ®µæ˜ å°„è‡ªåŠ¨è½¬æ¢ï¼š

```python
# åŸå§‹æ•°æ®
{"problem_id": "001", "code": "...", "expected": "..."}

# å­—æ®µæ˜ å°„
field_mapping = {
    "id": "problem_id",
    "problem": "code",
    "answer": "expected"
}
```

## ä½¿ç”¨ç¤ºä¾‹

### ä½å±‚çº§ï¼ˆç›´æ¥ä½¿ç”¨è¯„ä¼°å™¨ï¼‰

```python
from hello_agents import HelloAgentsLLM, UniversalLLMJudgeEvaluator
from hello_agents.evaluation.benchmarks.data_generation_Universal.evaluation_config import EvaluationConfig

# åˆå§‹åŒ–
llm = HelloAgentsLLM(provider="deepseek", model="deepseek-chat")
config = EvaluationConfig.load_template("math")
evaluator = UniversalLLMJudgeEvaluator(llm=llm, eval_config=config)

# è¯„ä¼°å•é¡¹
data = {"id": "001", "problem": "2+2=?", "answer": "4"}
result = evaluator.evaluate_single(data)
print(f"åˆ†æ•°: {result['total_score']}")
print(f"ç»´åº¦åˆ†æ•°: {result['scores']}")
```

### å¸¦å­—æ®µæ˜ å°„

```python
# æ•°æ®ä½¿ç”¨éæ ‡å‡†å­—æ®µå
data = {"item_id": "001", "code": "def add(a,b): return a+b", "expected": "sum"}

# æ·»åŠ å­—æ®µæ˜ å°„
field_mapping = {"id": "item_id", "problem": "code", "answer": "expected"}

evaluator = UniversalLLMJudgeEvaluator(
    llm=llm,
    eval_config=EvaluationConfig.load_template("code"),
    field_mapping=field_mapping
)

result = evaluator.evaluate_single(data)
```

### é«˜å±‚çº§ï¼ˆä½¿ç”¨å·¥å…·ï¼‰

```python
from hello_agents import HelloAgentsLLM, UniversalLLMJudgeTool

llm = HelloAgentsLLM(provider="deepseek", model="deepseek-chat")
tool = UniversalLLMJudgeTool(template="math", llm=llm)

# ä¸€è¡Œä»£ç å®Œæˆæ‰€æœ‰æ­¥éª¤
result = tool.run({
    "source_config": {"path": "data.json"},
    "field_mapping": {"problem": "problem", "answer": "answer"},
    "template": "math",
    "max_samples": 100,
    "output_dir": "results"
})
```

### Win Rate å¯¹æ¯”è¯„ä¼°

```python
from hello_agents import HelloAgentsLLM, UniversalWinRateEvaluator

llm = HelloAgentsLLM(provider="deepseek", model="deepseek-chat")
config = EvaluationConfig.load_template("writing")

field_mapping = {"problem": "content", "answer": "target"}

evaluator = UniversalWinRateEvaluator(
    llm=llm,
    eval_config=config,
    field_mapping=field_mapping
)

# å¯¹æ¯”ç”Ÿæˆæ•°æ®å’Œå‚è€ƒæ•°æ®
generated = [{"id": "g1", "content": "...", "target": "..."}]
reference = [{"id": "r1", "content": "...", "target": "..."}]

result = evaluator.evaluate_win_rate(generated, reference, num_comparisons=10)
print(f"èƒœç‡: {result['win_rate']}")
```

## APIå±‚çº§æ·±åº¦å¯¹æ¯”åˆ†æ

### è¯¦ç»†ç‰¹æ€§å¯¹æ¯”

| ç‰¹æ€§ | ä½å±‚çº§API | é«˜å±‚çº§API |
|------|-----------|-----------|
| **æ ¸å¿ƒç±»** | `UniversalLLMJudgeEvaluator`<br/>`UniversalWinRateEvaluator` | `UniversalLLMJudgeTool` |
| **çµæ´»æ€§** | â­â­â­â­â­ å®Œå…¨æ§åˆ¶ | â­â­â­ é¢„è®¾é…ç½® |
| **æ˜“ç”¨æ€§** | â­â­â­ éœ€è¦æ‰‹åŠ¨é…ç½® | â­â­â­â­â­ ä¸€é”®æ‰§è¡Œ |
| **æ•°æ®å¤„ç†** | æ‰‹åŠ¨åŠ è½½å’Œé¢„å¤„ç† | è‡ªåŠ¨åŠ è½½å’ŒéªŒè¯ |
| **ç»“æœè¾“å‡º** | æ‰‹åŠ¨å¤„ç†å’Œä¿å­˜ | è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Š |
| **é”™è¯¯å¤„ç†** | éœ€è¦è‡ªè¡Œå®ç° | å†…ç½®é™çº§ç­–ç•¥ |
| **æ‰¹é‡å¤„ç†** | æ‰‹åŠ¨å¾ªç¯å®ç° | è‡ªåŠ¨å¹¶è¡Œå¤„ç† |
| **è¿›åº¦è·Ÿè¸ª** | éœ€è¦è‡ªè¡Œå®ç° | å†…ç½®è¿›åº¦æ¡ |
| **é…ç½®ç®¡ç†** | å®Œå…¨å¯å®šåˆ¶ | é¢„è®¾æ¨¡æ¿é€‰æ‹© |

### ä½¿ç”¨åœºæ™¯å»ºè®®

#### é€‰æ‹©ä½å±‚çº§APIçš„æƒ…å†µï¼š

```python
# åœºæ™¯1: éœ€è¦ç²¾ç»†æ§åˆ¶è¯„ä¼°è¿‡ç¨‹
from hello_agents import HelloAgentsLLM, UniversalLLMJudgeEvaluator
from hello_agents.evaluation.benchmarks.data_generation_Universal.evaluation_config import EvaluationConfig

llm = HelloAgentsLLM(provider="deepseek", model="deepseek-chat")
custom_config = EvaluationConfig.custom(
    technical_quality="æŠ€æœ¯å®ç°è´¨é‡",
    innovation="åˆ›æ–°ç¨‹åº¦",
    feasibility="å¯è¡Œæ€§åˆ†æ"
)

evaluator = UniversalLLMJudgeEvaluator(llm=llm, eval_config=custom_config)

# å¯ä»¥å¯¹æ¯ä¸ªè¯„ä¼°æ­¥éª¤è¿›è¡Œç²¾ç»†æ§åˆ¶
def batch_evaluate_with_retry(data_list, max_retries=3):
    results = []
    for data in data_list:
        for attempt in range(max_retries):
            try:
                result = evaluator.evaluate_single(data)
                # è‡ªå®šä¹‰åå¤„ç†é€»è¾‘
                result['custom_score'] = calculate_custom_score(result)
                results.append(result)
                break
            except Exception as e:
                if attempt == max_retries - 1:
                    # è‡ªå®šä¹‰é”™è¯¯å¤„ç†
                    results.append(create_error_result(data, str(e)))
    return results
```

#### é€‰æ‹©é«˜å±‚çº§APIçš„æƒ…å†µï¼š

```python
# åœºæ™¯2: å¿«é€ŸåŸå‹å’ŒéªŒè¯
from hello_agents import HelloAgentsLLM, UniversalLLMJudgeTool

llm = HelloAgentsLLM(provider="deepseek", model="deepseek-chat")
tool = UniversalLLMJudgeTool(template="code", llm=llm)

# ä¸€è¡Œä»£ç å®Œæˆç«¯åˆ°ç«¯è¯„ä¼°
result = tool.run({
    "source_config": {"path": "code_samples.json"},
    "template": "code",
    "output_dir": "evaluation_results",
    "max_samples": 100,
    "field_mapping": {"problem": "question", "answer": "solution"}
})

print(f"è¯„ä¼°å®Œæˆï¼Œå¹³å‡åˆ†: {result['summary']['average_score']}")
```


### APIé€‰æ‹©å†³ç­–æ ‘

```
éœ€è¦è¯„ä¼°å¤§é‡æ•°æ®ï¼Ÿ
â”œâ”€ æ˜¯ â†’ éœ€è¦å¿«é€Ÿè·å¾—ç»“æœï¼Ÿ
â”‚   â”œâ”€ æ˜¯ â†’ ä½¿ç”¨é«˜å±‚çº§API (UniversalLLMJudgeTool)
â”‚   â””â”€ å¦ â†’ ä½¿ç”¨ä½å±‚çº§API (UniversalLLMJudgeEvaluator)
â””â”€ å¦ â†’ éœ€è¦è‡ªå®šä¹‰è¯„ä¼°ç»´åº¦ï¼Ÿ
    â”œâ”€ æ˜¯ â†’ ä½¿ç”¨ä½å±‚çº§API + è‡ªå®šä¹‰é…ç½®
    â””â”€ å¦ â†’ ä¸¤è€…çš†å¯ï¼Œé«˜å±‚çº§APIæ›´ç®€å•
```

## å­—æ®µæ˜ å°„æœ€ä½³å®è·µ

```python
# âœ… å¥½ï¼šæ¸…æ™°æ˜ å°„
field_mapping = {
    "id": "item_id",
    "problem": "question",
    "answer": "solution"
}

# âŒ å·®ï¼šå­—æ®µåä¸åŒ¹é…æˆ–ç¼ºå°‘
field_mapping = {
    "problem": "Question"  # æºæ•°æ®ä¸­å®é™…æ˜¯ "question"
}
```

## å¸¸è§é—®é¢˜

**Q: å¦‚ä½•è·å–æ¨¡æ¿çš„æ‰€æœ‰ç»´åº¦ï¼Ÿ**
```python
config = EvaluationConfig.load_template("math")
print(config.get_dimension_names())
```

**Q: å­—æ®µæ˜ å°„åŒºåˆ†å¤§å°å†™å—ï¼Ÿ**
æ˜¯çš„ï¼Œè¯·å‡†ç¡®åŒ¹é…æºæ•°æ®çš„å­—æ®µåã€‚

**Q: ç¼ºå°‘å¿…éœ€å­—æ®µä¼šæ€æ ·ï¼Ÿ**
è¯„ä¼°ä¼šå¤±è´¥ã€‚å¿…éœ€å­—æ®µï¼š`id`ã€`problem`ã€`answer`ã€‚

**Q: å¯ä»¥è‡ªå®šä¹‰è¯„ä¼°ç»´åº¦å—ï¼Ÿ**
å½“å‰ä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿ã€‚å¯ä¿®æ”¹ Prompt è¿›è¡Œé«˜çº§å®šåˆ¶ã€‚

## è¾“å‡ºæ ¼å¼

### LLM Judge ç»“æœ
```python
{
    "problem_id": "001",
    "scores": {
        "correctness": 4.5,
        "clarity": 4.0,
        ...
    },
    "total_score": 4.3
}
```

### Win Rate ç»“æœ
```python
{
    "win_rate": 0.55,
    "loss_rate": 0.30,
    "tie_rate": 0.15,
    "wins": 11,
    "losses": 6,
    "ties": 3
}
```

## å®æˆ˜æ¡ˆä¾‹å’Œæœ€ä½³å®è·µ

### æ¡ˆä¾‹1: æ•™è‚²å¹³å°ä½œä¸šè‡ªåŠ¨æ‰¹æ”¹

```python
# åœºæ™¯: åœ¨çº¿æ•™è‚²å¹³å°çš„æ•°å­¦ä½œä¸šè‡ªåŠ¨æ‰¹æ”¹ç³»ç»Ÿ
from hello_agents import HelloAgentsLLM, UniversalLLMJudgeEvaluator
from hello_agents.evaluation.benchmarks.data_generation_Universal.evaluation_config import EvaluationConfig

class MathGradingSystem:
    def __init__(self):
        self.llm = HelloAgentsLLM(provider="deepseek", model="deepseek-chat")
        # ä½¿ç”¨æ•°å­¦æ¨¡æ¿ï¼Œæ·»åŠ æ•™å­¦ç›¸å…³ç»´åº¦
        self.config = EvaluationConfig.custom(
            correctness="è§£é¢˜æ­¥éª¤å’Œç­”æ¡ˆçš„æ•°å­¦æ­£ç¡®æ€§",
            clarity="è§£é¢˜è¿‡ç¨‹çš„é€»è¾‘æ¸…æ™°åº¦å’Œè¡¨è¾¾è§„èŒƒ",
            completeness="è§£é¢˜æ­¥éª¤çš„å®Œæ•´æ€§å’Œè¯¦ç»†ç¨‹åº¦",
            methodology="è§£é¢˜æ–¹æ³•çš„åˆç†æ€§å’Œåˆ›æ–°æ€§",
            learning_outcome="ä½“ç°å‡ºçš„æ•°å­¦ç†è§£å’ŒæŒæ¡ç¨‹åº¦"
        )
        self.evaluator = UniversalLLMJudgeEvaluator(
            llm=self.llm,
            eval_config=self.config,
            field_mapping={
                "id": "student_id",
                "problem": "question",
                "answer": "student_answer"
            }
        )

    def grade_homework(self, homework_data):
        """æ‰¹æ”¹ä½œä¸šå¹¶ç”Ÿæˆè¯¦ç»†åé¦ˆ"""
        results = []

        for submission in homework_data:
            # åŸºç¡€è¯„ä¼°
            evaluation = self.evaluator.evaluate_single(submission)

            # ç”Ÿæˆä¸ªæ€§åŒ–åé¦ˆ
            feedback = self.generate_feedback(evaluation, submission)
            evaluation['feedback'] = feedback

            # è®¡ç®—æ”¹è¿›å»ºè®®
            if evaluation['total_score'] < 3.5:
                evaluation['improvement_tips'] = self.generate_improvement_tips(evaluation)

            results.append(evaluation)

        return results

    def generate_feedback(self, evaluation, submission):
        """æ ¹æ®è¯„ä¼°ç»“æœç”Ÿæˆåé¦ˆ"""
        scores = evaluation['scores']

        feedback = []
        if scores['correctness'] >= 4.0:
            feedback.append("âœ… ç­”æ¡ˆæ­£ç¡®ï¼Œè®¡ç®—æ— è¯¯")
        elif scores['correctness'] >= 3.0:
            feedback.append("âš ï¸ åŸºæœ¬æ­£ç¡®ï¼Œä½†å­˜åœ¨ä¸€äº›è®¡ç®—é”™è¯¯")
        else:
            feedback.append("âŒ ç­”æ¡ˆæœ‰è¯¯ï¼Œéœ€è¦é‡æ–°æ£€æŸ¥è®¡ç®—è¿‡ç¨‹")

        if scores['methodology'] >= 4.0:
            feedback.append("ğŸ¯ è§£é¢˜æ–¹æ³•å¾ˆå¥½ï¼Œæ€è·¯æ¸…æ™°")

        return " ".join(feedback)

    def generate_improvement_tips(self, evaluation):
        """ä¸ºä½åˆ†å­¦ç”Ÿç”Ÿæˆæ”¹è¿›å»ºè®®"""
        tips = []
        scores = evaluation['scores']

        if scores['clarity'] < 3.5:
            tips.append("å»ºè®®è¯¦ç»†å†™å‡ºæ¯ä¸€æ­¥çš„è®¡ç®—è¿‡ç¨‹")
        if scores['completeness'] < 3.5:
            tips.append("è¯·æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„è§£é¢˜æ­¥éª¤")

        return tips

# ä½¿ç”¨ç¤ºä¾‹
grading_system = MathGradingSystem()
homework_submissions = [
    {
        "student_id": "stu001",
        "question": "è§£æ–¹ç¨‹: 2xÂ² - 8x + 6 = 0",
        "student_answer": "ä½¿ç”¨æ±‚æ ¹å…¬å¼: x = [8 Â± âˆš(64-48)]/4 = [8 Â± 4]/4, æ‰€ä»¥ x = 3 æˆ– x = 1"
    }
]

results = grading_system.grade_homework(homework_submissions)
for result in results:
    print(f"å­¦ç”Ÿ{result['problem_id']}: {result['total_score']:.1f}åˆ†")
    print(f"åé¦ˆ: {result['feedback']}")
```



### æ¡ˆä¾‹2: å†…å®¹è¥é”€è´¨é‡è¯„ä¼°

```python
# åœºæ™¯: è¥é”€å›¢é˜Ÿå†…å®¹è´¨é‡è‡ªåŠ¨åŒ–è¯„ä¼°
from hello_agents import HelloAgentsLLM, UniversalWinRateEvaluator
from hello_agents.evaluation.benchmarks.data_generation_Universal.evaluation_config import EvaluationConfig

class ContentQualityEvaluator:
    def __init__(self):
        self.llm = HelloAgentsLLM(provider="deepseek", model="deepseek-chat")

        # ä¸“é—¨ç”¨äºè¥é”€å†…å®¹çš„è¯„ä¼°é…ç½®
        self.marketing_config = EvaluationConfig.custom(
            brand_alignment="å“ç‰Œè°ƒæ€§å¥‘åˆåº¦",
            audience_engagement="ç›®æ ‡å—ä¼—å¸å¼•åŠ›",
            value_proposition="ä»·å€¼ä¸»å¼ æ¸…æ™°åº¦",
            call_to_action="è¡ŒåŠ¨å¬å”¤æ•ˆæœ",
            seo_optimization="SEOä¼˜åŒ–ç¨‹åº¦",
            originality="å†…å®¹åŸåˆ›æ€§å’Œç‹¬ç‰¹æ€§"
        )

        self.evaluator = UniversalLLMJudgeEvaluator(
            llm=self.llm,
            eval_config=self.marketing_config
        )

    def benchmark_content(self, new_content, competitor_contents):
        """ä¸ç«å“å†…å®¹è¿›è¡Œå¯¹æ¯”è¯„ä¼°"""
        win_rate_evaluator = UniversalWinRateEvaluator(
            llm=self.llm,
            eval_config=self.marketing_config
        )

        # è®¡ç®—ç›¸å¯¹äºç«å“çš„èƒœç‡
        win_rate_result = win_rate_evaluator.evaluate_win_rate(
            generated_data=new_content,
            reference_data=competitor_contents,
            num_comparisons=10
        )

        # è¯¦ç»†è¯„ä¼°æ–°å†…å®¹
        detailed_evaluations = []
        for content in new_content:
            evaluation = self.evaluator.evaluate_single(content)
            detailed_evaluations.append(evaluation)

        return {
            'competitive_advantage': win_rate_result,
            'detailed_analysis': detailed_evaluations,
            'recommendations': self.generate_content_recommendations(detailed_evaluations)
        }

    def generate_content_recommendations(self, evaluations):
        """åŸºäºè¯„ä¼°ç»“æœç”Ÿæˆå†…å®¹æ”¹è¿›å»ºè®®"""
        recommendations = []

        # è®¡ç®—å„ç»´åº¦å¹³å‡åˆ†
        avg_scores = {}
        for evaluation in evaluations:
            for dimension, score in evaluation['scores'].items():
                if dimension not in avg_scores:
                    avg_scores[dimension] = []
                avg_scores[dimension].append(score)

        for dimension, scores in avg_scores.items():
            avg_score = sum(scores) / len(scores)
            if avg_score < 3.5:
                if dimension == 'brand_alignment':
                    recommendations.append("å»ºè®®åŠ å¼ºå“ç‰Œå…ƒç´ çš„ä½¿ç”¨ï¼Œç¡®ä¿ä¸å“ç‰Œè°ƒæ€§ä¿æŒä¸€è‡´")
                elif dimension == 'audience_engagement':
                    recommendations.append("å†…å®¹å¸å¼•åŠ›ä¸è¶³ï¼Œå»ºè®®å¢åŠ äº’åŠ¨å…ƒç´ å’Œæƒ…æ„Ÿå…±é¸£ç‚¹")
                elif dimension == 'call_to_action':
                    recommendations.append("è¡ŒåŠ¨å¬å”¤ä¸å¤Ÿæ˜ç¡®ï¼Œå»ºè®®åŠ å¼ºCTAçš„è®¾è®¡å’Œä½ç½®")

        return recommendations

# ä½¿ç”¨ç¤ºä¾‹
content_evaluator = ContentQualityEvaluator()

# æ–°è¥é”€å†…å®¹
new_articles = [
    {
        "id": "article_001",
        "problem": "æ’°å†™å…³äºäººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿çš„è¥é”€æ–‡ç« ",
        "answer": "AIæ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚ä»æ™ºèƒ½å®¶å±…åˆ°è‡ªåŠ¨é©¾é©¶..."
    }
]

# ç«å“å†…å®¹
competitor_articles = [
    {
        "id": "comp_001",
        "problem": "äººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿åˆ†æ",
        "answer": "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£ä»¥å‰æ‰€æœªæœ‰çš„é€Ÿåº¦å‘å±•ï¼Œæ·±åº¦å­¦ä¹ ..."
    }
]

benchmark_result = content_evaluator.benchmark_content(new_articles, competitor_articles)
print(f"ç›¸å¯¹ç«å“èƒœç‡: {benchmark_result['competitive_advantage']['metrics']['win_rate']:.2%}")
print("æ”¹è¿›å»ºè®®:", benchmark_result['recommendations'])
```




## é€šè¿‡æœ¬æŒ‡å—ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿï¼š
1. æ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚çš„è¯„ä¼°æ¨¡æ¿å’ŒAPIå±‚çº§
2. åˆ›å»ºè‡ªå®šä¹‰è¯„ä¼°ç»´åº¦å’Œä¸“ç”¨æ¨¡æ¿
3. å¤„ç†ä¸åŒæ ¼å¼çš„æ•°æ®æº
4. å®ç°é«˜è´¨é‡çš„è‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿ

